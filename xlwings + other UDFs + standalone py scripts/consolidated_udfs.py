"""
Consolidated xlwings UDF Module
Auto-generated by merge script
"""

from bs4 import BeautifulSoup
from cffi import FFI
from concurrent.futures import ThreadPoolExecutor
from ctypes import c_char_p, c_int, POINTER, byref
from dask.distributed import Client
#from datetime import datetime
#from datetime import datetime as dt
from datetime import datetime, timedelta
from decimal import Decimal
from io import StringIO
from javascript import require
from pathlib import Path
from py4j.java_gateway import JavaGateway
from randomgen import RDRAND
from rdrand import RdRandom
from rdrand import RdSeedom
from scipy.optimize import newton
from sympy import sin, cos, log, sqrt
from sympy import symbols, integrate
from ta.trend import sma_indicator
from ta.trend import wma_indicator
from typing import Optional, Dict, List, Any
import calendar
import cffi
import csv
import ctypes
import dask.dataframe as dd
#import datetime
import execjs
import json
import math
import numpy as np
import os
import pandas as pd
import pcre2
import pyarrow as pa
import pyarrow.compute as pc
import pyarrow.csv as csv
import rdrand
import re
import regex
import regex as re1
import requests
import sqlite3
import string
import subprocess
import sympy as sp
import sys
import threading
import time
import warnings
import xlwings as xw
import yfinance as yf



# ==================== FROM ss.py ====================
# Initialize FFI
ffi = FFI()
# Define the C function signature
ffi.cdef("""
    bool* evaluate_short_circuit(double *array1, double *array2, const char *logical_operator,
                                 const char *comp_op1, double var1, const char *comp_op2,
                                 double var2, int size, int use_const1, int use_const2, int num_threads);
""")
# Load the DLL (ensure the correct path to the DLL)
lib = ffi.dlopen(r'D:\dev\dll\short-circuit.dll')
@xw.func
@xw.arg('array1', ndim=1)  # Treat array1 as 1D array, even if a single element
@xw.arg('array2', ndim=1)  # Treat array2 as 1D array, even if a single element
def Short_Circuit(array1, array2, logical_op, comp1, is_const1, const1, comp2, is_const2, const2, num_threads):
    #if isinstance(array1, float):
    #    array1 = [array1]
    #if isinstance(array2, float):
    #    array2 = [array2]
    # Convert input arrays to float and handle None values
    array1 = [float(i) if i is not None else 0.0 for i in array1]
    array2 = [float(i) if i is not None else 0.0 for i in array2]
    # Convert Python lists to C arrays (double arrays)
    c_array1 = ffi.new("double[]", array1)
    c_array2 = ffi.new("double[]", array2)
    # Convert comparison constants and thread count to integers
    is_const1 = int(is_const1)
    is_const2 = int(is_const2)
    num_threads = int(num_threads)
    const1 = float(const1)
    const2 = float(const2)
    # Call the C function, which returns a pointer to a boolean array
    c_result = lib.evaluate_short_circuit(c_array1, c_array2,
                                          ffi.new("char[]", logical_op.encode()),
                                          ffi.new("char[]", comp1.encode()),
                                          const1,
                                          ffi.new("char[]", comp2.encode()),
                                          const2,
                                          len(array1),
                                          is_const1,
                                          is_const2,
                                          num_threads)
    # Convert the result from the C boolean array to a Python list
    output = [[c_result[i]] for i in range(len(array1))]
    # Return the output directly as a list of TRUE/FALSE values
    return output

# Initialize FFI for C function interaction
ffi1 = FFI()

# Load the DLL (Ensure the DLL path is correct)
dll = ffi1.dlopen(r"D:\dev\dll\short-circuit-simple.dll")

# Define the function prototype from the DLL
ffi1.cdef("""
    bool* evaluate_short_circuit_multithread(double* array1, double* array2, 
                                             const char* logical_op, const char* comp_op1, 
                                             const char* comp_op2, double constant1, 
                                             double constant2, int size, int num_threads);
""")

def evaluate_condition(val1, val2, condition, constant1=None, constant2=None):
    """
    Dynamically evaluate the given condition. Constants are used when comparison strings
    contain placeholders like 'constant1' or 'constant2'.
    """
    # Replace 'val1' and 'val2' with actual values
    condition = condition.replace('val1', str(val1))
    condition = condition.replace('val2', str(val2))

    # Use constant1 and constant2 if present in the condition string
    if 'constant1' in condition and constant1 is not None:
        condition = condition.replace('constant1', str(constant1))
    if 'constant2' in condition and constant2 is not None:
        condition = condition.replace('constant2', str(constant2))

    # Evaluate the condition dynamically
    try:
        return eval(condition)
    except Exception as e:
        raise ValueError(f"Error in condition evaluation: {condition}. Error: {str(e)}")

@xw.func
@xw.arg('array1', ndim=1)  # Input array1 as a standard Python list
@xw.arg('array2', ndim=1)  # Input array2 as a standard Python list
@xw.arg('logical_op', str)  # Logical operator (|| or &&)
@xw.arg('comp_op1', str)    # First comparison operation (e.g., 'val1 > constant1')
@xw.arg('comp_op2', str)    # Second comparison operation (e.g., 'val2 < constant2')
@xw.arg('constant1', float) # First constant
@xw.arg('constant2', float) # Second constant
@xw.arg('num_threads', int) # Number of threads for parallel execution
def short_circuit_simple(array1, array2, logical_op, comp_op1, comp_op2, constant1, constant2, num_threads):
    """
    Evaluate flexible comparisons between two arrays and constants using short-circuit logic.
    """
    try:
        # Ensure both arrays are of the same size
        size = len(array1)
        if size != len(array2):
            return "Array sizes do not match"

        # Convert Python lists to C-compatible arrays
        array1_c = ffi1.new("double[]", array1)
        array2_c = ffi1.new("double[]", array2)

        # Prepare results array for storing C results
        results_c = ffi1.new("bool[]", size)

        # Loop through each element to evaluate conditions
        for i in range(size):
            val1 = array1[i]
            val2 = array2[i]
            
            # Evaluate the first condition
            cond1 = evaluate_condition(val1, val2, comp_op1, constant1, constant2)
            # Evaluate the second condition
            cond2 = evaluate_condition(val1, val2, comp_op2, constant1, constant2)

            # Apply logical operation
            if logical_op == "||":
                results_c[i] = cond1 or cond2
            elif logical_op == "&&":
                results_c[i] = cond1 and cond2
            else:
                raise ValueError("Invalid logical operator. Use '||' or '&&'.")

        # Convert results from C array to Python list
        results = [[bool(results_c[i])] for i in range(size)]
        
        # Return the results as a list
        return results
    
    except Exception as e:
        # Return error message to Excel
        return f"Error: {str(e)}"

# ==================== FROM udf.py ====================
print(sys.executable)
@xw.func
@xw.arg('Lst', ndim=2)
def multiply_elements(Lst):
    # Separate lists and multiply each by 5
    new_Lst = [[(element*5)+3 for element in sub_list] for sub_list in Lst]
    return new_Lst
@xw.func
@xw.arg('Lst', ndim=2)
def multiply_elements_2(Lst):
    # Separate lists and multiply each by 5
    new_Lst = [[(element*4)+8 for element in sub_list] for sub_list in Lst]
    return new_Lst
@xw.func
@xw.arg('days_amounts', ndim=2)
def aging_buckets(days_amounts):
    aging_list = [
        (lambda day, amount: amount < 0, "Advance"),
        (lambda day, amount: 0 <= day < 31, "Not Due"),
        (lambda day, amount: 31 <= day < 61, "31-60 Days"),
        (lambda day, amount: 61 <= day < 91, "61-90 Days"),
        (lambda day, amount: 91 <= day < 181, "91-180 Days"),
        (lambda day, amount: 181 <= day < 366, "6 Months to 1 year"),
        (lambda day, amount: 366 <= day, "More than 1 year")
    ]
    result = [[next((label for condition, label in aging_list if condition(day[0], day[1])), 'No Match')] for day in days_amounts if len(day) == 2]
    return result
@xw.func
@xw.arg('days_amounts', ndim=2)
def aging_buckets_parallel(days_amounts):
    client = Client(n_workers=6, threads_per_worker=4)
    df = pd.DataFrame(days_amounts, columns=['days', 'amount'])
    df['days'] = pd.to_numeric(df['days'], errors='coerce')
    df['amount'] = pd.to_numeric(df['amount'], errors='coerce')
    ddf = dd.from_pandas(df, npartitions=4)
    aging_list = [
            (lambda day, amount: int(amount) < 0, "Advance"),
            (lambda day, amount: 0 <= int(day) < 31, "Not Due"),
            (lambda day, amount: 31 <= int(day) < 61, "31-60 Days"),
            (lambda day, amount: 61 <= int(day) < 91, "61-90 Days"),
            (lambda day, amount: 91 <= int(day) < 181, "91-180 Days"),
            (lambda day, amount: 181 <= int(day) < 366, "6 Months to 1 year"),
            (lambda day, amount: 366 <= int(day), "More than 1 year")
            ]
    ddf['age_bucket'] = ddf.map_partitions(lambda df: df.apply(lambda row: next((label for condition, label in aging_list if condition(row['days'], row['amount'])), None), axis=1), meta=('days', 'object'))
    result_df = ddf.compute()
    result = [[item] for item in result_df['age_bucket'].values]
    client.close()
    return result
@xw.func
@xw.arg('date_and_months', ndim=2)
def EOMONTHM(date_and_months):
    result = []
    for row in date_and_months:
        dt = row[0]
        month_offset = int(row[1])
        year, month = divmod(dt.month - 1 + month_offset, 12)
        _, last_day = calendar.monthrange(dt.year + year, month + 1)
        eomonth = datetime(dt.year + year, month + 1, last_day)
        result.append([(eomonth - datetime(1899, 12, 30)).days])
    return result
@xw.func
@xw.arg('date_and_months', ndim=2)
def EDATEM(date_and_months):
    result = []
    for row in date_and_months:
        # The date is already a datetime.datetime object
        dt = row[0]
        month_offset = int(row[1])
        # Add the month offset to the current month and adjust the year if necessary
        year, month = divmod(dt.month - 1 + month_offset, 12)
        edate = datetime(dt.year + year, month + 1, dt.day)
        # Convert the datetime.datetime object back to an Excel date (ordinal)
        result.append([(edate - datetime(1899, 12, 30)).days])
    return result
@xw.func
@xw.arg('numbers', ndim=2)
def check_duplicates(numbers):
    # Flatten the list of lists
    numbers = [num for sublist in numbers for num in sublist]
    if len(numbers) == len(set(numbers)):
        return "No duplicates found."
    else:
        return "Duplicates found."
@xw.func
def RandomGennp(n):
    # Convert n to an integer
    n = int(n)
    # Create a new RDRAND generator
    rg = RDRAND()
    # Generate n random numbers, each composed of three 5-digit numbers
    numbers = [int((rg.random_raw() % 90000 + 10000) * 1e10 + (rg.random_raw() % 90000 + 10000) * 1e5 + (rg.random_raw() % 90000 + 10000)) for _ in range(n)]
    # Convert the list to a list of lists for xlwings
    numbers_list = [[number] for number in numbers]
    # Return the list of lists
    return numbers_list
rng = rdrand.RdRandom()
@xw.func
def generate_random_numbers_rdrand(num):
    # Generate the random numbers
    random_numbers = [[rng.randint(10**14, 10**15 - 1)] for _ in range(int(num))]
    return random_numbers
@xw.func
@xw.arg('x_values', ndim=2)
@xw.arg('z_values', ndim=2)
@xw.arg('target', numbers=float)
def wa_return(x_values, z_values, target):
    # Convert input lists of lists to numpy arrays
    x_values = np.array(x_values)
    z_values = np.array(z_values)
    # Define the function for which we want to find the root
    def func(y):
        return np.sum(x_values * ((1 + y / 4) ** z_values)) - target
    # Use the Newton-Raphson method to find the root
    y_initial_guess = 0.5
    y_solution = newton(func, y_initial_guess)
    return y_solution
@xw.func
def intel_rdrand_boost(NUM_NUMBERS, NUM_WORKERS, NUM_THREADS):
    NUM_NUMBERS = int(NUM_NUMBERS)
    NUM_WORKERS = int(NUM_WORKERS)
    NUM_THREADS = int(NUM_THREADS)
    numbers_ptr = dll.generateRandomNumbers(NUM_NUMBERS, NUM_WORKERS, NUM_THREADS)
    numbers = [[int(numbers_ptr[i])] for i in range(NUM_NUMBERS)]
    return numbers
@xw.func
@xw.arg('dates', ndim=2)
def date_buckets(dates):
    df = pd.DataFrame(dates, columns=['date'])
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    date_list = [
            (lambda date: (date.month == 4 and date.day >= 1) or (date.month == 5) or (date.month == 6 and date.day <= 15), "01/04 To 15/06"),
            (lambda date: (date.month == 6 and date.day >= 16) or (date.month == 7) or (date.month == 8) or (date.month == 9 and date.day <= 15), "16/06 To 15/09"),
            (lambda date: (date.month == 9 and date.day >= 16) or (date.month == 10) or (date.month == 11) or (date.month == 12 and date.day <= 15), "16/09 To 15/12"),
            (lambda date: (date.month == 12 and date.day >= 16) or (date.month == 1) or (date.month == 2) or (date.month == 3 and date.day <= 15), "16/12 To 15/03"),
            (lambda date: (date.month == 3 and date.day >= 16 and date.day <= 31), "16/03 To 31/03")
            ]
    df['bucket'] = df['date'].apply(lambda date: next((label for condition, label in date_list if condition(date)), None))
    result = [[item] for item in df['bucket'].values]
    return result
@xw.func
def convert_timestamps(timestamps):
    # Initialize an empty list to store the converted dates
    converted_dates = []
    # Check if timestamps is a list of lists
    if all(isinstance(i, list) for i in timestamps):
        # Iterate over each list in the input range
        for row in timestamps:
            # Iterate over each timestamp in the row
            for timestamp in row:
                # Convert the timestamp to an integer
                timestamp = int(timestamp)
                # Convert the Unix timestamp to a datetime object
                dt_object = datetime.fromtimestamp(timestamp)
                # Format the datetime object as a string in the format 'dd/mm/yyyy'
                date_string = dt_object.strftime('%d/%m/%Y')
                # Add the converted date to the converted_dates list as a single-item list
                converted_dates.append([date_string])
    else:
        # If timestamps is a list of floats, convert each timestamp
        for timestamp in timestamps:
            timestamp = int(timestamp)
            dt_object = datetime.fromtimestamp(timestamp)
            date_string = dt_object.strftime('%d/%m/%Y')
            # Add the converted date to the converted_dates list as a single-item list
            converted_dates.append([date_string])
    # Return the list of lists with the converted dates
    return converted_dates
@xw.func
def get_stock_data_yf(symbol, start_date, end_date):
    # Convert the date format from "dd/mm/yyyy" to "yyyy-mm-dd"
    start_date = datetime.datetime.strptime(start_date, "%d/%m/%Y").strftime("%Y-%m-%d")
    end_date = datetime.datetime.strptime(end_date, "%d/%m/%Y").strftime("%Y-%m-%d")
    # Download historical market data
    hist = yf.Ticker(symbol).history(start=start_date, end=end_date)
    # Fill missing values with a default value (like 'N/A' or 0)
    hist.fillna('N/A', inplace=True)
    # Reset the index to include it in the output
    hist.reset_index(inplace=True)
    # Convert the DataFrame to a list of lists
    data = hist.values.tolist()
    # Add the column names as the first list in the output
    data.insert(0, hist.columns.tolist())
    return data
@xw.func
def get_url_data_new(symbol, from_date, to_date):
    end_date_timestamp = int(dt.strptime(to_date, "%d/%m/%Y").timestamp())
    start_date_timestamp = int(dt.strptime(from_date, "%d/%m/%Y").timestamp())
    url = f'http://api.scraperlink.com/investpy/?email=asharindani51@gmail.com&url=https%3A%2F%2Fin.investing.com%2Fequities%2F{symbol}-historical-data%3Fend_date%3D{end_date_timestamp}%26st_date%3D{start_date_timestamp}'
    Response = requests.get(url)
    time.sleep(5)
    soup = BeautifulSoup(Response.content, 'html.parser')
    div = soup.find('div', {'class': 'common-table-scroller js-table-scroller'})
    table = div.find('table', {'class': 'common-table medium js-table'})
    colgroup = table.find('colgroup')
    headers = [col.get('class')[0] for col in colgroup.find_all('col')]
    tbody = table.find('tbody')
    data = [
            [
                dt.strptime(" ".join([td.text.strip().rsplit(' ', 1)[0], re.sub(r'\D', '', td.text.strip().rsplit(' ', 1)[1])]), "%b %d, %Y") if i == 0 else float(td.text.replace(',', '')) if 1 <= i <= 4 else td.text.strip() 
                for i, td in enumerate(tr.find_all('td'))
                ]# .strftime("%d/%m/%Y") in dt.strptime(" ".join([td.text.strip().rsplit(' ', 1)[0], re.sub(r'\D', '', td.text.strip().rsplit(' ', 1)[1])]), "%b %d, %Y") at the end deleted to avoid date being converted to text.
            for tr in tbody.find_all('tr')
            ]
    result = [headers] + data
    return result
@xw.func
def get_url_data_id(investingid,from_date, to_date):
    from_date = datetime.datetime.strptime(from_date, "%d/%m/%Y").strftime("%Y-%m-%d")
    to_date = datetime.datetime.strptime(to_date, "%d/%m/%Y").strftime("%Y-%m-%d")
    investingid=int(investingid)
    url = f'http://api.scraperlink.com/investpy/?email=asharindani51@gmail.com&url=https%3A%2F%2Fapi.investing.com%2Fapi%2Ffinancialdata%2Fhistorical%2F{investingid}%3Fstart-date%3D{from_date}%26end-date%3D{to_date}%26time-frame%3DDaily%26add-missing-rows%3Dfalse'
    response = requests.get(url)
    json_output = response.json()  # This is already a Python dictionary
    data_list = json_output['data']
    data = []
    if data_list:
        # Add headers as the first row in data
        headers = list(data_list[0].keys())
        data.append(headers)
        for item in data_list:
            if item is not None:
                data.append(list(item.values()))
    return data
@xw.func
def get_url_data_id_new(investingid, from_date, to_date):
    # Convert date strings to the required format
    from_date = datetime.strptime(from_date, "%d/%m/%Y").strftime("%Y-%m-%d")
    to_date = datetime.strptime(to_date, "%d/%m/%Y").strftime("%Y-%m-%d")
    # Construct the URL
    url = f'http://api.scraperlink.com/investpy/?email=asharindani51@gmail.com&amp;url=https%3A%2F%2Fapi.investing.com%2Fapi%2Ffinancialdata%2Fhistorical%2F{investingid}%3Fstart-date%3D{from_date}%26end-date%3D{to_date}%26time-frame%3DDaily%26add-missing-rows%3Dfalse'
    # Fetch data from the API
    response = requests.get(url)
    json_output = response.json()
    data_list = json_output['data']
    # Convert data_list into a pandas dataframe
    df = pd.DataFrame(data_list)
    # Remove commas from integer and float columns
    int_float_columns = ['last_close','last_open','last_max','last_min','change_precent','last_closeRaw', 'last_openRaw', 'last_maxRaw', 'last_minRaw', 'change_precentRaw']
    for col in int_float_columns:
        df[col] = df[col].astype(str).str.replace(',', '').astype('float')
    # Format columns
    df['direction_color'] = df['direction_color'].astype(str)
    df['rowDate'] = pd.to_datetime(df['rowDate'], format='%b %d, %Y').apply(lambda x: x.date())
    df['rowDateTimestamp'] = pd.to_datetime(df['rowDateTimestamp']).apply(lambda x: x.date())
    # Keep only the columns you want
    columns_to_keep = ['direction_color', 'rowDate', 'rowDateRaw', 'rowDateTimestamp', 'last_close', 'last_open', 'last_max', 'last_min', 'volume', 'volumeRaw', 'change_precent']
    df = df[columns_to_keep]
    # Create a matrix (list of lists) from the dataframe
    matrix = df.values.tolist()
    # Add headers at the beginning of the list
    headers = df.columns.tolist()
    matrix.insert(0, headers)
    return matrix
@xw.func
@xw.arg('excel_range', ndim=2)
@xw.arg('patterns', ndim=1)
@xw.arg('replacement')
def REGEXREPLACE(excel_range, patterns, replacement):
    result = []
    if replacement is None:
        replacement = ""
    for row in excel_range:
        row_result = []
        for cell in row:
            cell_result = cell
            for pattern in patterns:
                if re.search(pattern, cell_result):
                    cell_result = re.sub(pattern, replacement, cell_result)
            row_result.append(cell_result)
        result.append(row_result)
    return result
@xw.func
@xw.arg('dates', ndim=2)
def quarter_buckets(dates):
    df = pd.DataFrame(dates, columns=['date'])
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    quarter_list = [
            (lambda date: 4 <= date.month <= 6, "Q1"),
            (lambda date: 7 <= date.month <= 9, "Q2"),
            (lambda date: 10 <= date.month <= 12, "Q3"),
            (lambda date: 1 <= date.month <= 3, "Q4")
            ]
    df['quarter'] = df['date'].apply(lambda date: next((label for condition, label in quarter_list if condition(date)), None))
    result = [[item] for item in df['quarter'].values]
    return result
@xw.func
@xw.arg('excel_range', ndim=2)
@xw.arg('patterns', ndim=1)
def REGEXFINDM(excel_range, patterns):
    result = []
    has_any_match = False
    
    for row in excel_range:
        row_result = []
        for cell in row:
            if cell is None or str(cell).strip() == "":
                row_result.append("")
                continue
                
            cell_str = str(cell)
            cell_result = []
            for pattern in patterns:
                match = re.search(pattern, cell_str, flags=re.UNICODE)
                if match:
                    cell_result.append(cell_str)
            
            if len(cell_result) == len(patterns):
                row_result.append(" ".join(cell_result))
                has_any_match = True
            else:
                row_result.append("")
        result.append(row_result)
    
    # Return empty if no matches found
    if not has_any_match:
        return [[""]]
    
    return result
@xw.func
def fetch_crude_data(start_date, end_date):
    # Convert the dates from "dd/mm/yyyy" to "yyyy-mm-dd"
    start_date = datetime.strptime(start_date, "%d/%m/%Y").strftime("%Y-%m-%d")
    end_date = datetime.strptime(end_date, "%d/%m/%Y").strftime("%Y-%m-%d")
    # Read the API key from the file
    with open('d:\\dev\\apikey.txt', 'r') as file:
        api_key = file.read().replace('\n', '')
    # Define the API URL
    url = f"https://api.eia.gov/v2/petroleum/pri/spt/data/?frequency=daily&data[0]=value&facets[product][]=EPCBRENT&start={start_date}&end={end_date}&sort[0][column]=period&sort[0][direction]=desc&offset=0&length=5000&api_key={api_key}"
    # Send a GET request to the API
    response = requests.get(url)
    # Convert the response to JSON
    data = json.loads(response.text)
    # Extract the nested data under the "data" key
    nested_data = data['response']['data']
    # Convert the nested data to a pandas DataFrame
    df = pd.json_normalize(nested_data)
    # Convert the date to "dd/mm/yyyy" format
    df['period'] = pd.to_datetime(df['period']).dt.strftime('%d/%m/%Y')
    # Convert the 'value' column to float
    df['value'] = df['value'].astype(float)
    # Convert the DataFrame to a 2D list, including headers
    data_list = [df.columns.values.tolist()] + df.values.tolist()
    return data_list
@xw.func
def fetch_gold_data(start_date, end_date):
    # Convert the dates from "dd/mm/yyyy" to "yyyy-mm-dd"
    start_date = datetime.strptime(start_date, "%d/%m/%Y").strftime("%Y-%m-%d")
    end_date = datetime.strptime(end_date, "%d/%m/%Y").strftime("%Y-%m-%d")
    # Read the API key from the file
    with open('d:\\dev\\apikey2.txt', 'r') as file:
        api_key = file.read().replace('\n', '')
    # Define the API URL
    url = f"https://api.metalpriceapi.com/v1/timeframe?api_key={api_key}&start_date={start_date}&end_date={end_date}&base=XAU&currencies=INR"
    # Send a GET request to the API
    response = requests.get(url)
    # Convert the response to JSON
    data = json.loads(response.text)
    # Extract the nested data under the "rates" key
    nested_data = data['rates']
    # Convert the nested data to a list of dictionaries
    data_list = [{'date': date, 'INR': rate['INR']} for date, rate in nested_data.items()]
    # Convert the list of dictionaries to a pandas DataFrame
    df = pd.DataFrame(data_list)
    # Convert the date to "dd/mm/yyyy" format
    df['date'] = pd.to_datetime(df['date']).dt.strftime('%d/%m/%Y')
    # Convert the 'INR' column to float
    df['INR'] = df['INR'].astype(float)
    # Convert the DataFrame to a 2D list, including headers
    data_list = [df.columns.values.tolist()] + df.values.tolist()
    return data_list
@xw.func
def fetch_commodity_data(base, start_date, end_date):
    # Convert the dates from "dd/mm/yyyy" to "yyyy-mm-dd"
    start_date = datetime.strptime(start_date, "%d/%m/%Y").strftime("%Y-%m-%d")
    end_date = datetime.strptime(end_date, "%d/%m/%Y").strftime("%Y-%m-%d")
    # Read the API key from the file
    with open('d:\\dev\\apikey2.txt', 'r') as file:
        api_key = file.read().replace('\n', '')
    # Define the API URL
    url = f"https://api.metalpriceapi.com/v1/timeframe?api_key={api_key}&start_date={start_date}&end_date={end_date}&base={base}&currencies=INR"
    # Send a GET request to the API
    response = requests.get(url)
    # Convert the response to JSON
    data = json.loads(response.text)
    # Extract the nested data under the "rates" key
    nested_data = data['rates']
    # Convert the nested data to a list of dictionaries
    data_list = [{'date': date, 'INR': rate['INR']} for date, rate in nested_data.items()]
    # Convert the nested data to a list of dictionaries, skipping dates without 'INR'
    #data_list = [{'date': date, 'INR': rate['INR']} for date, rate in nested_data.items() if 'INR' in rate]
    # Convert the list of dictionaries to a pandas DataFrame
    df = pd.DataFrame(data_list)
    # Convert the date to "dd/mm/yyyy" format
    df['date'] = pd.to_datetime(df['date']).dt.strftime('%d/%m/%Y')
    # Convert the 'INR' column to float
    df['INR'] = df['INR'].astype(float)
    # Convert the DataFrame to a 2D list, including headers
    data_list = [df.columns.values.tolist()] + df.values.tolist()
    return data_list
@xw.func
def generate_random_numbers_rdseed(n):
    s = RdSeedom()
    random_numbers = []
    n = int(n)
    while len(random_numbers) < n:
        # Generate a random integer with exactly 15 digits
        num = s.randint(100000000000000, 999999999999999)
        # Append the number as a list to create a list of lists
        random_numbers.append([num])
    return random_numbers
    return output_list
url='https://chartered.tax/fmv-of-equity-share-as-on-31st-january-2018/'
@xw.func
def get_shares_prices():
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    table = soup.find('table', {'id': 'tablepress-4'})
    table_io = StringIO(str(table))
    df = pd.read_html(table_io)[0]
    return df.values.tolist()
@xw.func
@xw.arg('excel_range', ndim=2)
@xw.arg('patterns', ndim=1)
def REGEXLATEST(excel_range, patterns):
    result = []
    for row in excel_range:
        row_result = []
        for cell in row:
            cell_str = str(cell)  # Convert cell to string
            for pattern in patterns:
                if re.search(pattern, cell_str):
                    row_result.append(cell_str)
                    break
            else:
                row_result.append("")
        result.append(row_result)
    return result

@xw.func
def RDSEED_EXCEL(min_val, max_val, count):
    count=int(count)
    random_numbers_rdseed = []
    for _ in range(count):
        random_number_rdseed = rdrand.RdSeedom().get_bits(64) % (max_val - min_val + 1) + min_val
        random_numbers_rdseed.append(random_number_rdseed)
    return [[random_number_rdseed_1] for random_number_rdseed_1 in random_numbers_rdseed]



# ==================== FROM udf2.py ====================
ffi = cffi.FFI()
ffi.cdef("""
    int match_pattern_in_array(const char **input_array, int array_length, const char *pattern, const char ***output_array);
    void free_matches(const char **matches, int match_count);
""")
# Load the DLL
os.environ['PATH'] = r'D:\Programs\msys64\home\juhi\Downloads' + ';' + os.environ['PATH']+';'+r'D:\Programs\msys64\ucrt64\lib'+';'+r'D:\Programs\msys64\ucrt64\bin'
dll = ffi.dlopen("regex_cpp.dll")
@xw.func
def match_pattern_pcre2(input_list, pattern):
    try:
        # Convert input strings into a C array
        input_array = [ffi.new("char[]", (item or "").encode('utf-8')) for item in input_list]
        input_array_c = ffi.new("char*[]", input_array)
        
        # Prepare output array (pointer to pointer)
        output_array_c = ffi.new("const char***")  # triple pointer
        
        # Call the DLL function
        match_count = dll.match_pattern_in_array(input_array_c, len(input_list), pattern.encode('utf-8'), output_array_c)
        
        # If the match count is negative, return an empty list
        if match_count < 0:
            return [[""] for _ in input_list]  # Return an empty list vertically
        
        # Convert output back to Python
        output_list = [ffi.string(output_array_c[0][i]).decode('utf-8') for i in range(match_count)]
        
        # Create a result list that matches the original input length
        final_output = [[item] if item in output_list else [""] for item in input_list]  # Vertical output (column)
        
        # Free the output array allocated by the DLL
        dll.free_matches(output_array_c[0], match_count)
        
        return final_output
    except Exception as e:
        print(f"Error: {e}")
        return [[""] for _ in input_list]  # Return vertical empty list on error

@xw.func
@xw.arg('excel_range', ndim=2)
@xw.arg('patterns', ndim=1)
def REGEXFINDGROUP(excel_range, patterns):
    result = []
    for row in excel_range:
        row_result = []
        for cell in row:
            cell_str = str(cell)  # Convert cell to string
            cell_result = []
            for pattern in patterns:
                match = re.search(pattern, cell_str)
                if match:
                    cell_result.append(match.group(0))  # Extract the captured group
            if len(cell_result) == len(patterns):
                row_result.append(" ".join(cell_result))
            else:
                row_result.append("")
        result.append(row_result)
    return result
@xw.func
@xw.arg('excel_range', ndim=2)
@xw.arg('patterns', ndim=1)
def REGEXFINDM2(excel_range, patterns):
    result = []
    for row in excel_range:
        row_result = []
        for cell in row:
            cell_str = str(cell)  # Convert cell to string
            cell_result = []
            for pattern in patterns:
                match = re.search(pattern, cell_str, flags=re.UNICODE)
                if match:
                    cell_result.append(cell_str)  # Return the entire string
            if len(cell_result) == len(patterns) and not cell_str.isprintable():
                row_result.append(" ".join(cell_result))
            else:
                row_result.append("")
        result.append(row_result)
    return result
@xw.func
@xw.arg('excel_range', ndim=2)
@xw.arg('patterns', ndim=1)
#@xw.arg('replacement', ndim=0)
def REGEXREPLM(excel_range, patterns, replacement):
    result = []
    for row in excel_range:
        row_result = []
        for cell in row:
            cell_str = str(cell)  # Convert cell to string
            for pattern in patterns:
                cell_str = re.sub(pattern, replacement, cell_str)  # Replace matched string with the specified replacement
            row_result.append(cell_str.strip())  # Remove leading/trailing spaces
        result.append(row_result)
    return result
# Define the integration function
def indefinite_integrate(func):
    x = sp.symbols('x')
    # Convert the string function to a sympy expression
    expr = sp.sympify(func)
    # Perform the indefinite integration
    result = sp.integrate(expr, x)
    return result

# Define the symbol x
x = symbols('x')

@xw.func
@xw.arg('excel_range', xw.Range)  # Use xw.Range to get the actual range object
def integrate_excel(excel_range):
    # Get the address of the range directly
    range_address = excel_range.address
    # Convert Excel range to list of lists
    func_list = excel_range.value  # Use .value to get the data from the range
    # Initialize results list
    results = []
    # Check if the range contains a single cell or multiple cells
    if len(excel_range) != 1:
        # Get the address of the first cell in the range
        first_cell_address = range_address.split(":")[0].replace("$", "")
        # Get the address of the last cell in the range
        last_cell_address = range_address.split(":")[1].replace("$", "")
        # Check if the range is horizontal or vertical
        for func in func_list:
            try:
                result = integrate(func, x)  # Each func is already a single value
                result_str = str(result).replace('**', '^')
                results.append(result_str)
            except Exception as e:
                results.append(f"Error: {e}")
        if first_cell_address[0] != last_cell_address[0]:
            # Horizontal range
            return [results]  # Return results as a horizontal array
        else:
            # Vertical range
            return [[res] for res in results]  # Return results as a vertical array
    else:
        try:
            result = integrate(func_list, x)  # Each func is already a single value
            result_str = str(result).replace('**', '^')
            results.append(result_str)
        except Exception as e:
            results.append(f"Error: {e}")
        # Single cell range
        return [results]

@xw.func
@xw.arg('data', ndim=2)
def calculate_tithi_excel(data):
    results = []
    for row in data:
        # Unpack the inputs from the row
        date, sun_degree, sun_minute, moon_degree, moon_minute = row
        try:
            # Debug: Print input values to verify correct reception
#            print(f"Date: {date}, Sun Degree: {sun_degree}, Sun Minute: {sun_minute}, Moon Degree: {moon_degree}, Moon Minute: {moon_minute}")
            # Check if the date is a datetime object or an Excel date float
            if isinstance(date, datetime) or isinstance(date, (float, int)):  
                # If it's a float or int, convert it to datetime
                if isinstance(date, (float, int)):
                    # Excel's base date is 1899-12-30, so we add 2 days
                    date_as_datetime = datetime.fromordinal(int(date) + 693594)
                else:
                    date_as_datetime = date  # already a datetime object
#                print(f"Converted Date: {date_as_datetime}")
                # Check if sun and moon degree/minute values are numeric
                if all(isinstance(x, (int, float)) for x in [sun_degree, sun_minute, moon_degree, moon_minute]):
                    # Combine degrees and minutes for Sun and Moon
                    sun_longitude = sun_degree + (sun_minute / 60)
                    moon_longitude = moon_degree + (moon_minute / 60)
                    # Calculate the difference in longitude
                    longitude_difference = moon_longitude - sun_longitude
                    # If the difference is negative, add 360 to make it positive
                    if longitude_difference < 0:
                        longitude_difference += 360
                    # Calculate the tithi number
                    tithi_number = int(longitude_difference // 12) + 1
                    # Determine the paksha and tithi name
                    if tithi_number <= 15:
                        paksha = 'Shukla'
                    else:
                        paksha = 'Krishna'
                    tithi_names = [
                        'Pratipada', 'Dwitiya', 'Tritiya', 'Chaturthi', 'Panchami',
                        'Shashthi', 'Saptami', 'Ashtami', 'Navami', 'Dashami',
                        'Ekadashi', 'Dwadashi', 'Trayodashi', 'Chaturdashi', 'Purnima', 'Amavasya'
                    ]
                    # Adjust the tithi index for Krishna Paksha
                    tithi_index = (tithi_number - 1) % 15
                    # Special case for Amavasya in Krishna Paksha
                    if paksha == 'Krishna' and tithi_number == 30:
                        tithi_name = tithi_names[-1]  # Amavasya
                    else:
                        tithi_name = tithi_names[tithi_index]
                    # Append the result for the current row
                    results.append(f"{paksha} {tithi_name}")
                else:
                    # If any of the sun or moon values are not numeric, return an error
                    results.append("#INVALID_SUN_OR_MOON_VALUE")
            else:
                # If date is not valid, append an error message
                results.append("#INVALID_DATE")
        except Exception as e:
            # Catch any other exceptions and return an error message
            results.append(f"#ERROR: {str(e)}")
    return results
@xw.func
@xw.arg('rng1', ndim=2)
@xw.arg('rng2', ndim=2)
def tstrangecalc(rng1, rng2):
    # Convert single-cell ranges to 2D lists
    if not isinstance(rng1[0], list):
        rng1 = [[cell for cell in rng1]]
    if not isinstance(rng2[0], list):
        rng2 = [[rng2[0][0]] * len(rng1[0])]  # Repeat the value to match dimensions
    # Perform element-wise addition
    result1 = [[rng1[i][j] + rng2[i][j] for j in range(len(rng1[0]))] for i in range(len(rng1))]
    return result1
@xw.func
#@xw.arg('data1', ndim=2)
@xw.arg('data', ndim=2)
def calculate_tithi_excel_new(data):
    results = []
    for row in data:
        try:
            # Check if the row has exactly 7 elements
#            if len(row) != 7:
#                results.append("#DIMENSION_ERROR")
#                continue
            # Unpack the inputs from the row
            date, sun, moon = row
            # Debug: Print input values to verify correct reception
#            print(f"Date: {date}, Sun: {sun}, Moon: {moon}")
            # Check if the date is a datetime object or an Excel date float
            if isinstance(date, datetime) or isinstance(date, (float, int)):
                # If it's a float or int, convert it to datetime
                if isinstance(date, (float, int)):
                    # Excel's base date is 1899-12-30, so we add 2 days
                    date_as_datetime = datetime.fromordinal(int(date) + 693594)
                else:
                    date_as_datetime = date  # already a datetime object
#                print(f"Converted Date: {date_as_datetime}")
                # Combine degrees for Sun and Moon
                sun_longitude = sun
                moon_longitude = moon
                # Calculate the difference in longitude
                longitude_difference = moon_longitude - sun_longitude
                # If the difference is negative, add 360 to make it positive
                if longitude_difference < 0:
                    longitude_difference += 360
                # Calculate the tithi number
                tithi_number = int(longitude_difference // 12) + 1
                # Determine the paksha and tithi name
                if tithi_number <= 15:
                    paksha = 'Shukla'
                else:
                    paksha = 'Krishna'
                tithi_names = [
                    'Pratipada', 'Dwitiya', 'Tritiya', 'Chaturthi', 'Panchami',
                    'Shashthi', 'Saptami', 'Ashtami', 'Navami', 'Dashami',
                    'Ekadashi', 'Dwadashi', 'Trayodashi', 'Chaturdashi', 'Purnima', 'Amavasya'
                ]
                # Adjust the tithi index for Krishna Paksha
                tithi_index = (tithi_number - 1) % 15
                # Special case for Amavasya in Krishna Paksha
                if paksha == 'Krishna' and tithi_number == 30:
                    tithi_name = tithi_names[-1]  # Amavasya
                else:
                    tithi_name = tithi_names[tithi_index]
                # Append the result for the current row
                results.append(f"{paksha} {tithi_name}")
            else:
                # If date is not valid, append an error message
                results.append("#INVALID_DATE")
        except Exception as e:
            # Catch any other exceptions and return an error message
            results.append(f"#ERROR: {str(e)}")
    return results


# ==================== FROM udf3.py ====================
@xw.func
@xw.arg('input_data', ndim=2)
def get_panchang(input_data):
    # Read API key from file
    with open(r"D:\dev\api_key_freeastrologyapicom.txt", "r") as file:
        api_key = file.read().strip()
    # Set API endpoint
    url = "https://json.freeastrologyapi.com/tithi-durations"
    # Process input data
    date_time_int = input_data[0][0]
    latitude = input_data[0][1]
    longitude = input_data[0][2]
    timezone = input_data[0][3]
    # Convert Excel date to datetime object
    dt = date_time_int
    # Extract year, month, date, hours, minutes, seconds
    year = dt.year
    month = dt.month
    date = dt.day
    hours = dt.hour
    minutes = dt.minute
    seconds = dt.second
    # Set payload
    payload = json.dumps({
        "year": year,
        "month": month,
        "date": date,
        "hours": hours,
        "minutes": minutes,
        "seconds": seconds,
        "latitude": latitude,
        "longitude": longitude,
        "timezone": timezone,
        "config": {
            "observation_point": "topocentric",
            "ayanamsha": "lahiri"
        }
    })
    # Set headers with API key
    headers = {
        'Content-Type': 'application/json',
        'x-api-key': api_key
    }
    # Send POST request
    response = requests.request("POST", url, headers=headers, data=payload)
    # Return response as JSON array
    intoutput=response.json()
    panchang = json.loads(intoutput["output"])
    headings = list(panchang.keys())
    values = list(panchang.values())
    completes_at_index = headings.index("completes_at")
    date_string = values[completes_at_index]
    date_object = datetime.strptime(date_string, "%Y-%m-%d %H:%M:%S")
    formatted_date = date_object.strftime("%d-%m-%Y %H:%M:%S")
    values[completes_at_index] = formatted_date
    return [headings, values]


# ==================== FROM udf4.py ====================
@xw.func
@xw.arg('input_data_2', ndim=2)
def get_sunrise_sunset(input_data_2):
#    del date, latitude, longitude, timezone, response, payload, data, key_values
    # Read API endpoint
    print(input_data_2)
    url = "https://api.sunrisesunset.io/json"
    # Process input data
    date1 = input_data_2[0][0]
    latitude = input_data_2[0][1]
    longitude = input_data_2[0][2]
    timezone = input_data_2[0][3]
    if isinstance(date1, (int, float)):
        date = datetime(1899, 12, 30) + timedelta(days=date1)
    else:
        date = date1
    # Set payload
    payload = {
        "lat": latitude,
        "lng": longitude,
        "date": date,
        "timezone": timezone
    }
    # Send GET request
    response = requests.get(url, params=payload)
    # Load JSON response
    print(response.json())
    data = response.json()
    keys_values = data["results"]
    sunrise_key = next(key for key in keys_values if key == "sunrise")
    sunset_key = next(key for key in keys_values if key == "sunset")
    # Extract sunrise and sunset timings
    sunrise = data["results"]["sunrise"]
    sunset = data["results"]["sunset"]
    sunrise_time = datetime.strptime(sunrise, '%I:%M:%S %p').time()
    sunset_time = datetime.strptime(sunset, '%I:%M:%S %p').time()
        # Combine date and time
    if isinstance(date, str):
        date_obj = datetime.strptime(date, '%Y-%m-%d').date()
    else:
        date_obj = date.date()
    sunrise_dt = datetime.combine(date_obj, sunrise_time)
    sunset_dt = datetime.combine(date_obj, sunset_time)
     # Prepare the result
    resultnew1 = [
        ["Key", "Value"],
        [sunrise_key, sunrise_dt],
        [sunset_key, sunset_dt]
    ]
    return resultnew1

# ==================== FROM udf5.py ====================
# Excel UDF to calculate tithi
@xw.func(async_mode='threading', volatile=False)
def tithinew(date_input):
    # Convert Excel date to datetime object
#    date_as_datetime = datetime.datetime.fromordinal(int(date_input) + 693594)
    # Format datetime object as string
    date_string = date_input.strftime("%d/%m/%Y %H:%M:%S")
    # Replace date string in sun-moon-elongj2000.ssc file
    with open(r"D:\Programs\Stellarium\scripts\sun-moon-elongj2000.ssc", "r") as file:
        lines = file.readlines()
    for i, line in enumerate(lines):
        if 'var date1' in line:
            lines[i] = f'var date1 = "{date_string}";\n'
            break
    with open(r"D:\Programs\Stellarium\scripts\sun-moon-elongj2000.ssc", "w") as file:
        file.writelines(lines)
    # Change to the correct directory
    os.chdir(r"D:\Programs\Stellarium\scripts")
    # Run curl command
    os.system('curl -d "id=sun-moon-elongj2000.ssc" http://localhost:8090/api/scripts/run')
    time.sleep(4)
    # Parse JSON string from sun-moon.txt file
    with open(r"C:\Users\dhawal123\AppData\Roaming\Stellarium\sun-moon.txt", "r") as file:
        json_string = file.read()
    data = json.loads(json_string)
    time.sleep(2)
    # Extract Sun and Moon's parameters
    sun_longitude = data["Sun"]["elongJ2000"]
    moon_longitude = data["Moon"]["elongJ2000"]
    # Calculate tithi number and name
    longitude_difference = moon_longitude - sun_longitude
    if longitude_difference < 0:
        longitude_difference += 360
    tithi_number = int(longitude_difference // 12) + 1
    paksha = 'Shukla' if tithi_number <= 15 else 'Krishna'
    tithi_names = ['Pratipada', 'Dwitiya', 'Tritiya', 'Chaturthi', 'Panchami', 'Shashthi', 'Saptami', 'Ashtami', 'Navami', 'Dashami', 'Ekadashi', 'Dwadashi', 'Trayodashi', 'Chaturdashi', 'Purnima', 'Amavasya']
    tithi_index = (tithi_number - 1) % 15
    # Special case for Amavasya in Krishna Paksha
    if paksha == 'Krishna' and tithi_number == 30:
        tithi_name = tithi_names[-1]  # Amavasya
    else:
        tithi_name = tithi_names[tithi_index]
#    tithi_name = tithi_names[tithi_index]
    return f"{paksha} {tithi_name}"
@xw.func(volatile=False)
@xw.arg('input_data', ndim=2)
def lunar_month(input_data):
    print(input_data)
    # Read API key from file
    with open(r"D:\dev\api_key_freeastrologyapicom.txt", "r") as file:
        api_key = file.read().strip()
    # Set API endpoint
    url = "https://json.freeastrologyapi.com/lunarmonthinfo"
    # Process input data
    date_time_int = input_data[0][0]
    latitude = input_data[0][1]
    longitude = input_data[0][2]
    timezone = input_data[0][3]
    if isinstance(date_time_int, (int, float)):
        date_time_int = datetime(1899, 12, 30) + timedelta(days=date_time_int)
#    else:
#        date_time_int = date_time_int
    # Convert Excel date to datetime object
    dt = date_time_int
    # Extract year, month, date, hours, minutes, seconds
    year = dt.year
    month = dt.month
    date = dt.day
    hours = dt.hour
    minutes = dt.minute
    seconds = dt.second
    # Set payload
    payload = json.dumps({
        "year": year,
        "month": month,
        "date": date,
        "hours": hours,
        "minutes": minutes,
        "seconds": seconds,
        "latitude": latitude,
        "longitude": longitude,
        "timezone": timezone,
        "config": {
            "observation_point": "topocentric",
            "ayanamsha": "lahiri"
        }
    })
    # Set headers with API key
    headers = {
        'Content-Type': 'application/json',
        'x-api-key': api_key
    }
    # Send POST request
    response = requests.request("POST", url, headers=headers, data=payload)
    print(response.json())
   # Create a dictionary mapping original month names to desired names
    month_name_mapping = {
        "Pushyam": "Paush",
        "Maagham": "Maha",
        "Phalgunam": "Phagan",
        "Chaitram": "Chitra",
        "Vaisakham": "Vaishakh",
        "Jyeshtam": "Jeth",
        "Ashadam": "Ashadh",
        "Sravanam": "Shravan",
        "Bhadrapadam": "Bhadarvo",
        "Ashweeyujam": "Aaso",
        "Karthikam": "Kartak",
        "Maargasiram": "Magshar"
    }
    month_index_mapping = {
        10: 3,
        11: 4,
        12: 5,
        1: 6,
        2: 7,
        3: 8,
        4: 9,
        5: 10,
        6: 11,
        7: 12,
        8: 1,
        9: 2
    }
    # Return response as JSON array
    intoutput=response.json()
    panchang = json.loads(intoutput["output"])
    panchang["lunar_month_name"] = month_name_mapping[panchang["lunar_month_name"]]
    panchang["lunar_month_number"] = month_index_mapping[panchang["lunar_month_number"]]
    panchang["lunar_month_full_name"] = panchang["lunar_month_name"]
    headings = list(panchang.keys())
    values = list(panchang.values())
    return [headings, values]



# ==================== FROM udf6.py ====================
os.environ['PATH'] = r'D:\Programs\msys64\ucrt64\bin;D:\dev\dll'+';' + os.environ['PATH']
@xw.func
def weaverage(cls1, win123, fillna=False):
    """
    Calculate the Weighted Moving Average (WMA) for the given close prices.

    Parameters:
        cls1 (list of lists): Input range from Excel.
        win123 (int): Window size for WMA calculation.
        fillna (bool): Fill NaN values with False.

    Returns:
        list: Weighted Moving Average (WMA) series.
    """
    # Convert the list of lists to a pandas Series
    close_series = pd.Series([float(x) for x in cls1 if isinstance(x, (int, float))])
    
    # Calculate the WMA
    wma = wma_indicator(close_series, window=int(win123), fillna=fillna)
    
    # Return the WMA as a list
    return wma.tolist()

@xw.func
def smavg(cls2, win456, fillna=False):
    """
    Calculate the Weighted Moving Average (WMA) for the given close prices.

    Parameters:
        cls1 (list of lists): Input range from Excel.
        win123 (int): Window size for WMA calculation.
        fillna (bool): Fill NaN values with False.

    Returns:
        list: Weighted Moving Average (WMA) series.
    """
    # Convert the list of lists to a pandas Series
    close_series2 = pd.Series([float(x) for x in cls2 if isinstance(x, (int, float))])
    
    # Calculate the WMA
    sma = sma_indicator(close_series2, window=int(win456), fillna=fillna)
    
    # Return the WMA as a list
    return sma.tolist()
@xw.func
def slice_text_udf(input_range):
    # Flatten the input range (list of lists) into a single string
    input_string = ''.join([str(cell) for row in input_range for cell in row])
    # Function to slice the text into chunks of 32 characters
    def slice_text(text, chunk_size):
        return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
    # Slice the input string into chunks of 32 characters
    chunks = slice_text(input_string, 32)
    # Return the chunks as a vertical array (dynamic array)
    return [[chunk] for chunk in chunks]
@xw.func
def hex_to_decimal_string_udf(input_range_1):
        # Flatten the input range (list of lists) into a single list of strings
    flattened_input = input_range_1
#    flattened_input = [str(cell1).strip() for row1 in input_range_1 for cell1 in row1]
#    print(flattened_input)
    # Function to convert a single hex string to a decimal string
    def hex_to_decimal_string(hex_string_1):
        hex_string_1 = hex_string_1.strip()  # Remove any leading or trailing whitespace
        if len(hex_string_1) != 32:
            return "Error: Input must be 32 characters long."
        else:
            decimal_value_1 = sum(int(char_1, 16) * (16 ** i_1) for i_1, char_1 in enumerate(reversed(hex_string_1)))
            return str(decimal_value_1)
    # Iterate over each element in the flattened input and convert to decimal string
    result_1 = []
    for hex_string_1 in flattened_input:
        if hex_string_1:  # Ensure the cell is not empty after stripping
            decimal_string_1 = hex_to_decimal_string(hex_string_1).strip()
            # Append the entire decimal string as a single element in the result list
            result_1.append([decimal_string_1])
        else:
            result_1.append(["Error: Empty cell"])
    return result_1
@xw.func
def decimal_to_bits_udf(input_range_3):
    print(input_range_3)
    # Function to convert a single decimal number to the number of bits required
    def decimal_to_bits(decimal_number_input):
        try:
            decimal_number_3 = int(decimal_number_input)
        except ValueError:
            return "Invalid input"
        return str(math.ceil(math.log2(decimal_number_3)))
    # Flatten the input range (list of lists) into a single list of strings
    flattened_input_2 = input_range_3 
    # Iterate over each element in the flattened input and convert to number of bits
    result = []
    for decimal_number_2 in flattened_input_2:
        bits = decimal_to_bits(decimal_number_2)
        result.append([bits])
    return result
@xw.func
def hex_to_bits_udf(input_range_3):
    # Function to convert a single hex string to the number of bits required
    def hex_to_bits(hex_string_3):
        try:
            # Convert hex string to decimal number
            decimal_number_3 = int(hex_string_3, 16)
        except ValueError:
            return "Invalid input"
        # Calculate the number of bits required
        return str(math.ceil(math.log2(decimal_number_3)))
    # Flatten the input range (list of lists) into a single list of strings
    flattened_input_3 = input_range_3
    # Iterate over each element in the flattened input and convert to number of bits
    result = []
    for hex_string_3 in flattened_input_3:
        bits = hex_to_bits(hex_string_3)
        result.append([bits])
    return result





# ==================== FROM udf-dll.py ====================
os.environ['PATH'] = r'D:\Programs\msys64\ucrt64\bin;D:\dev\dll;D:\boost\libs'+';' + os.environ['PATH']

# Create FFI object
ffi1 = FFI()
# Define the C declarations
ffi1.cdef("""
    void generateRandomNumbersC(int numNumbers, int numThreadGroups, int numThreadsPerGroup);
    unsigned long long* getNumbersC();
    int getNumbersSizeC();
""")
# Load the DLL
dll1 = ffi1.dlopen('boost_rdseed_ucrt_new.dll')
@xw.func
def intel_rdrand_boost(NUM_NUMBERS, NUM_THREAD_GROUPS, NUM_THREADS_PER_GROUP):
    # Convert input parameters to integers
    NUM_NUMBERS = int(NUM_NUMBERS)
    NUM_THREAD_GROUPS = int(NUM_THREAD_GROUPS)
    NUM_THREADS_PER_GROUP = int(NUM_THREADS_PER_GROUP)
    # Call the functions
    dll1.generateRandomNumbersC(NUM_NUMBERS, NUM_THREAD_GROUPS, NUM_THREADS_PER_GROUP)
    numbers_ptr = dll1.getNumbersC()
    numbers_size = dll1.getNumbersSizeC()
    # Get the numbers
    numbers = [[int(numbers_ptr[i])] for i in range(numbers_size)]
    return numbers
# Create FFI object
ffi2 = FFI()
# Define the C declarations
ffi2.cdef("""
    void generateRandomNumbersC(int numNumbers, int numThreadGroups, int numThreadsPerGroup);
    unsigned long long* getNumbersC();
    int getNumbersSizeC();
""")
# Load the DLL
dll2 = ffi2.dlopen('boost_rdseed_ucrt_clang_new.dll')
@xw.func
def intel_rdrand_boost_clang(NUM_NUMBERS, NUM_THREAD_GROUPS, NUM_THREADS_PER_GROUP):
    # Convert input parameters to integers
    NUM_NUMBERS = int(NUM_NUMBERS)
    NUM_THREAD_GROUPS = int(NUM_THREAD_GROUPS)
    NUM_THREADS_PER_GROUP = int(NUM_THREADS_PER_GROUP)
    # Call the functions
    dll2.generateRandomNumbersC(NUM_NUMBERS, NUM_THREAD_GROUPS, NUM_THREADS_PER_GROUP)
    numbers_ptr = dll2.getNumbersC()
    numbers_size = dll2.getNumbersSizeC()
    # Get the numbers
    numbers = [[int(numbers_ptr[i])] for i in range(numbers_size)]
    return numbers
# Load the DLL
dll3 = ctypes.CDLL('D:\\dev\\dll\\boost_rdseed_ucrt_clang_new.dll')
# Define the function types
dll3.generateRandomNumbersC.argtypes = [ctypes.c_int, ctypes.c_int, ctypes.c_int]
dll3.getNumbersC.restype = ctypes.POINTER(ctypes.c_ulonglong)
dll3.getNumbersSizeC.restype = ctypes.c_int
@xw.func
def intel_ctypes_clang_dll(NUM_NUMBERS, NUM_THREAD_GROUPS, NUM_THREADS_PER_GROUP):
    # Convert input parameters to integers
    NUM_NUMBERS = int(NUM_NUMBERS)
    NUM_THREAD_GROUPS = int(NUM_THREAD_GROUPS)
    NUM_THREADS_PER_GROUP = int(NUM_THREADS_PER_GROUP)
    # Call the functions
    dll3.generateRandomNumbersC(NUM_NUMBERS, NUM_THREAD_GROUPS, NUM_THREADS_PER_GROUP)
    numbers_ptr = dll3.getNumbersC()
    numbers_size = dll3.getNumbersSizeC()
    # Get the numbers
    numbers = [[numbers_ptr[i]] for i in range(numbers_size)]
    return numbers
# Load the DLL
dll5 = ctypes.CDLL('D:\\dev\\dll\\boost_rdseed_ucrt_new.dll')
# Define the function types
dll5.generateRandomNumbersC.argtypes = [ctypes.c_int, ctypes.c_int, ctypes.c_int]
dll5.getNumbersC.restype = ctypes.POINTER(ctypes.c_ulonglong)
dll5.getNumbersSizeC.restype = ctypes.c_int
@xw.func
def intel_ctypes_dll(NUM_NUMBERS, NUM_THREAD_GROUPS, NUM_THREADS_PER_GROUP):
    # Convert input parameters to integers
    NUM_NUMBERS = int(NUM_NUMBERS)
    NUM_THREAD_GROUPS = int(NUM_THREAD_GROUPS)
    NUM_THREADS_PER_GROUP = int(NUM_THREADS_PER_GROUP)
    # Call the functions
    dll5.generateRandomNumbersC(NUM_NUMBERS, NUM_THREAD_GROUPS, NUM_THREADS_PER_GROUP)
    numbers_ptr = dll5.getNumbersC()
    numbers_size = dll5.getNumbersSizeC()
    # Get the numbers
    numbers = [[numbers_ptr[i]] for i in range(numbers_size)]
    return numbers
@xw.func
@xw.arg('excel_range', ndim=2)
@xw.arg('patterns', ndim=1)
def REGEXFIND(excel_range, patterns):
    result = []
    for row in excel_range:
        row_result = []
        for cell in row:
            cell_result = []
            for pattern in patterns:
                match = regex.search(pattern, cell)
                if match:
                    cell_result.append(match.group())
            if len(cell_result) == len(patterns):
                row_result.append(" ".join(cell_result))
            else:
                row_result.append("Pattern Not Found")
        result.append(row_result)
    return result
ffi = FFI()
# Load the DLL
#lib = ffi.dlopen('D:/Downloads/rust_dll/target/release/rust_rand_dll_copilot.dll')
dll_dir = r'D:\Downloads\rust_dll\target\release'
os.environ['PATH'] = dll_dir + ';' + os.environ['PATH']
lib = ffi.dlopen('rust_rand_dll_copilot_parallel.dll')
# Define the C signatures of the functions
ffi.cdef("""
    int rdrand64_step(unsigned long long *rand);
    void generate_random_numbers(int num_threads, int num_numbers);
    void allocate_memory(int num_numbers);
    unsigned long long* get_numbers();
    void free_memory();
""")
@xw.func
@xw.arg('num_threads', numbers=int)  # Use @xw.arg to specify the type of the argument
@xw.arg('num_numbers', numbers=int)
def rust_dll_rdrand(num_threads, num_numbers):
    # Allocate memory for the numbers
    lib.allocate_memory(num_numbers)
    # Generate random numbers
    lib.generate_random_numbers(num_threads, num_numbers)
    # Retrieve the generated numbers
    numbers_ptr = lib.get_numbers()
    numbers = [[numbers_ptr[i]] for i in range(num_numbers)]  # Return as list of lists
    # Free the allocated memory
    lib.free_memory()
    return numbers
# Create FFI object
ffi100 = FFI()
# Define the C declarations
ffi100.cdef("""
    void generateRandomNumbersC(int numNumbers, int numThreadGroups, int numThreadsPerGroup);
    unsigned long long* getNumbersC();
    int getNumbersSizeC();
""")
# Load the DLL
dll100 = ffi100.dlopen('boost_rdseed_ucrt_new_vc.dll')
@xw.func
def intel_rdseed_boost_vc(NUM_NUMBERS, NUM_THREAD_GROUPS, NUM_THREADS_PER_GROUP):
    # Convert input parameters to integers
    NUM_NUMBERS = int(NUM_NUMBERS)
    NUM_THREAD_GROUPS = int(NUM_THREAD_GROUPS)
    NUM_THREADS_PER_GROUP = int(NUM_THREADS_PER_GROUP)
    # Call the functions
    dll100.generateRandomNumbersC(NUM_NUMBERS, NUM_THREAD_GROUPS, NUM_THREADS_PER_GROUP)
    numbers_ptr = dll100.getNumbersC()
    numbers_size = dll100.getNumbersSizeC()
    # Get the numbers
    numbers = [[int(numbers_ptr[i])] for i in range(numbers_size)]
    return numbers


# ==================== FROM java-test-udf.py ====================
@xw.func
def convert_array_to_string(input_array):
    # Start the gateway
    gateway = JavaGateway()
    # Access the Java class
    converter = gateway.jvm.num2str()
    # Handle the case where input is None
    if input_array is None:
        return [[""]]
    # Check if the input is a single value or a range
    if isinstance(input_array, (float, int, str)):
        input_array = [[input_array]]  # Convert single value to a 2D array
    # Convert the input array to a list of lists if necessary
    if isinstance(input_array, list):
        if isinstance(input_array[0], list):
            flat_array = [item for sublist in input_array for item in sublist]
        else:
            flat_array = [item for item in input_array]
    else:
        # Handle case where input_array is not iterable
        flat_array = []
    # Define the array in Java (using Object[] to allow mixed types)
    elements = gateway.new_array(gateway.jvm.Object, len(flat_array))
    for i in range(len(flat_array)):
        elements[i] = flat_array[i]
    # Call the Java method
    java_result = converter.convertArrayToString(elements)
    # Convert the Java result array to a Python list
    result = []
    for item in java_result:
        if item.startswith("\u200B"):
            item = item[1:]  # Remove invisible character
        result.append(item)
    # Convert the result to a vertical format for Excel
    vertical_result = [[item] for item in result]
    # Shut down the gateway
    gateway.close()
    return vertical_result
@xw.func
def java_sqrt(input_array):
    gateway = JavaGateway()
    sqrt_func = gateway.jvm.Math.sqrt
    # Handle single-cell input
    if not isinstance(input_array, list):
        input_array = [input_array]
    result = []
    for value in input_array:
        try:
            if isinstance(value, (int, float)) and value >= 0:
                result.append(sqrt_func(value))
            else:
                result.append("Error")
        except Exception:
            result.append("Error")
    # Convert to vertical format for Excel
    result = [[item] for item in result]
    gateway.close()
    return result
@xw.func
def java_abs(input_array):
    gateway = JavaGateway()
    abs_func = gateway.jvm.Math.abs
    # Handle single-cell input
    if not isinstance(input_array, list):
        input_array = [input_array]
    result = []
    for value in input_array:
        try:
            if isinstance(value, (int, float)):
                result.append(abs_func(value))
            else:
                result.append("Error")
        except Exception:
            result.append("Error")
    # Convert to vertical format for Excel
    result = [[item] for item in result]
    gateway.close()
    return result
@xw.func
def java_ln(input_array):
    gateway = JavaGateway()
    ln_func = gateway.jvm.Math.log
    # Handle single-cell input
    if not isinstance(input_array, list):
        input_array = [input_array]
    result = []
    for value in input_array:
        try:
            if isinstance(value, (int, float)) and value > 0:
                result.append(ln_func(value))
            else:
                result.append("Error")
        except Exception:
            result.append("Error")
    # Convert to vertical format for Excel
    result = [[item] for item in result]
    gateway.close()
    return result

@xw.func
def next_after_arrays(array1, array2):
    # Convert inputs to lists of floats
    def convert_to_float_list(array):
        if isinstance(array, (float, int, str)):
            return [float(array)]
        elif isinstance(array, list):
            return [float(item) for item in array]
        return []

    flat_array1 = convert_to_float_list(array1)
    flat_array2 = convert_to_float_list(array2)

    # Ensure both arrays have the same length
    if len(flat_array1) != len(flat_array2):
        raise ValueError("Both input arrays must have the same length.")

    # Prepare the Java command with arguments
    java_command = ["java", "NextAfterTest"] + [
        f"{item}" for pair in zip(flat_array1, flat_array2) for item in pair
    ]

    try:
        # Set working directory to where NextAfterTest.class is located
        working_dir = os.path.dirname(__file__)  # Adjust as needed if the .class file is elsewhere

        # Run the Java command and capture the output
        result = subprocess.run(
            java_command,
            capture_output=True,
            text=True,
            cwd=working_dir,  # Ensures the Java class is found
        )

        # Check for errors in execution
        if result.returncode != 0:
            raise RuntimeError(f"Java process failed: {result.stderr.strip()}")

        # Capture Java output, convert it to numbers, and return as a vertical list
        java_output = result.stdout.strip().split('\n')
        numeric_output = [float(item) for item in java_output]
        return [[num] for num in numeric_output]  # Return numbers for Excel

    except Exception as e:
        return [[f"Error: {str(e)}"]]


# ==================== FROM udf7.py ====================
@xw.func
def FILL_EMPTY(excel_range):
    filled_list = []
    last_value = ""
    for cell in excel_range:
        if cell is not None and cell != "":  # Check if the cell is not None or empty
            last_value = cell
        filled_list.append([last_value])  # Ensure each result is a list to maintain vertical format
    return filled_list
@xw.func
@xw.arg('data', ndim=1)
def SPLIT_TEXT(data, delimiter):
    try:
        if not data or all(cell is None for cell in data):
            return [""]
        # Split the text and find the maximum length of the sublists
        split_data = [cell.split(delimiter) if cell is not None else [""] for cell in data]
        max_length = max(len(sublist) for sublist in split_data)
        # Ensure all sublists have the same length by padding with empty strings
        padded_data = [sublist + [""] * (max_length - len(sublist)) for sublist in split_data]
        return padded_data
    except Exception as e:
        return str(e)
@xw.func
def EXCELORM(range1, range2, condition):
    result = []
    for val1, val2 in zip(range1, range2):
        val1 = 0 if val1 is None or val1 == "" else float(val1)
        val2 = 0 if val2 is None or val2 == "" else float(val2)
        # Use eval to dynamically evaluate the condition
        if eval(condition):
            result.append([True])
        else:
            result.append([False])
    return result
@xw.func
def get_sanskrit_mantras(url):
    # Send a GET request to the webpage
    response = requests.get(url)
    # Parse the HTML content of the webpage
    soup = BeautifulSoup(response.content, 'html.parser')
    # Find all article tags with the specified class
    articles = soup.find_all('article', class_='dpLyricsWrapper')
    # Initialize a list to store the mantras
    mantras = []
    # Iterate over each article tag
    for article in articles:
        # Find all div tags with the specified class within the article
        divs = article.find_all('div', class_='dpNameCardMantra dpFlexEqual')
        for div in divs:
            # Find the div containing the Sanskrit mantra
            mantra_div = div.find('div', class_='dpListPrimaryTitle')
            if mantra_div:
                # Extract the text and add it to the list
                mantras.append(mantra_div.get_text(strip=True))
    # Return the list of mantras in a vertical format
    return [[mantra] for mantra in mantras]
@xw.func
def EXCELXORM(range1, range2, condition1, condition2):
    result = []
    for val1, val2 in zip(range1, range2):
        # Handle empty or None values as 0
        val1 = 0 if val1 is None or val1 == "" else float(val1)
        val2 = 0 if val2 is None or val2 == "" else float(val2)
        # Replace 'val1' and 'val2' in the condition strings with actual values
        condition1_eval = condition1.replace('val1', str(val1)).replace('val2', str(val2))
        condition2_eval = condition2.replace('val1', str(val1)).replace('val2', str(val2))
        # Evaluate the conditions dynamically using eval
        cond1_result = eval(condition1_eval)
        cond2_result = eval(condition2_eval)
        # Perform XOR: True if exactly one condition is True, False otherwise
        if (cond1_result and not cond2_result) or (not cond1_result and cond2_result):
            result.append([True])
        else:
            result.append([False])
    return result
def generate_password(length=12):
    r = RdRandom()
    char_set = string.ascii_letters + string.digits + '?@$#^&*'
    special_chars = '?@$#^&*'
    # Ensure at least one uppercase, one lowercase, and one special character
    password = [
        r.choice(string.ascii_uppercase),
        r.choice(string.ascii_lowercase),
        r.choice(special_chars)
    ]
    # Fill the rest of the password length with random characters
    while len(password) < length:
        char = r.choice(char_set)
        # Ensure no more than two special characters
        if char in special_chars and sum(c in special_chars for c in password) >= 2:
            continue
        password.append(char)
    # Shuffle to avoid predictable patterns using rdrand
    for i in range(len(password)):
        j = r.randint(0, len(password) - 1)
        password[i], password[j] = password[j], password[i]
    # Ensure the password does not start with a special character
    while password[0] in special_chars:
        for i in range(len(password)):
            j = r.randint(0, len(password) - 1)
            password[i], password[j] = password[j], password[i]
    return ''.join(password)
@xw.func
def PASSRDRAND(dummy=None):
    return generate_password()


# Dictionaries for number to words conversion
units = {0: "", 1: "One", 2: "Two", 3: "Three", 4: "Four", 5: "Five", 6: "Six", 7: "Seven", 8: "Eight", 9: "Nine"}
teens = {11: "Eleven", 12: "Twelve", 13: "Thirteen", 14: "Fourteen", 15: "Fifteen", 16: "Sixteen", 17: "Seventeen", 18: "Eighteen", 19: "Nineteen"}
tens = {10: "Ten", 20: "Twenty", 30: "Thirty", 40: "Forty", 50: "Fifty", 60: "Sixty", 70: "Seventy", 80: "Eighty", 90: "Ninety"}
thousands = {1_000: "Thousand", 100_000: "Lakh", 10_000_000: "Crore"}

def number_to_words(n):
    if n in units:
        return units[n]
    if n in teens:
        return teens[n]
    if n in tens:
        return tens[n]
    
    for key in sorted(thousands.keys(), reverse=True):
        if n >= key:
            higher = n // key
            lower = n % key
            return number_to_words(higher) + " " + thousands[key] + ((" " + number_to_words(lower)) if lower > 0 else "")
    
    if n < 100:
        return tens[(n // 10) * 10] + " " + units[n % 10]
    if n < 1000:
        return units[n // 100] + " Hundred " + number_to_words(n % 100)
    
@xw.func
def INRWORDS(numbers):
    # Handle the case where numbers is a single element
    if isinstance(numbers, (int, float)):
        numbers = [numbers]

    results = []
    for number in numbers:
        rupees = int(number)
        paise = round((number - rupees) * 100)
        if rupees == 0 and paise == 0:
            word = "Rupees Zero And Paise Zero Only"
        elif rupees == 0 and paise > 0:
            word = f"Paise {number_to_words(paise)} Only"
        else:
            word = f"Rupee {number_to_words(rupees)}" if rupees == 1 else f"Rupees {number_to_words(rupees)}"
            if paise > 0:
                word += f" And Paise {number_to_words(paise)} Only"
            else:
                word += " Only"
        results.append(word)
    
    # Transpose the results to a vertical array
    return [[result] for result in results]
def generate_password_rdseed(length1=12):
    r1 = RdSeedom()
    char_set1 = string.ascii_letters + string.digits + '?@$#^&*'
    special_chars1 = '?@$#^&*'
    # Ensure at least one uppercase, one lowercase, and one special character
    password1 = [
        r1.choice(string.ascii_uppercase),
        r1.choice(string.ascii_lowercase),
        r1.choice(special_chars1)
    ]
    # Fill the rest of the password length with random characters
    while len(password1) < length1:
        char1 = r1.choice(char_set1)
        # Ensure no more than two special characters
        if char1 in special_chars1 and sum(c1 in special_chars1 for c1 in password1) >= 2:
            continue
        password1.append(char1)
    # Shuffle to avoid predictable patterns using rdrand
    for i1 in range(len(password1)):
        j1 = r1.randint(0, len(password1) - 1)
        password1[i1], password1[j1] = password1[j1], password1[i1]
    # Ensure the password does not start with a special character
    while password1[0] in special_chars1:
        for i1 in range(len(password1)):
            j1 = r1.randint(0, len(password1) - 1)
            password1[i1], password1[j1] = password1[j1], password1[i1]
    return ''.join(password1)
@xw.func
def PASSRDSEED(dummy=None):
    return generate_password_rdseed()


def generate_passwords(num_passwords, length1=12):
    num_passwords = int(num_passwords)
    r1 = RdSeedom()
    char_set1 = string.ascii_letters + string.digits + '?@$#^&*'
    special_chars1 = '?@$#^&*'
    
    def generate_password():
        # Ensure at least one uppercase, one lowercase, and two unique special characters
        password1 = [
            r1.choice(string.ascii_uppercase),
            r1.choice(string.ascii_lowercase),
            r1.choice(special_chars1)
        ]
        used_special_chars = set(password1[-1])
        
        # Add a second unique special character
        while len(used_special_chars) < 2:
            char1 = r1.choice(special_chars1)
            if char1 not in used_special_chars:
                password1.append(char1)
                used_special_chars.add(char1)
        
        # Fill the rest of the password length with random characters
        while len(password1) < length1:
            char1 = r1.choice(char_set1)
            # Ensure no more than two special characters
            if char1 in special_chars1 and char1 in used_special_chars:
                continue
            password1.append(char1)
        
        # Shuffle to avoid predictable patterns using rdrand
        for i1 in range(len(password1)):
            j1 = r1.randint(0, len(password1) - 1)
            password1[i1], password1[j1] = password1[j1], password1[i1]
        
        # Ensure the password does not start with a special character
        while password1[0] in special_chars1:
            for i1 in range(len(password1)):
                j1 = r1.randint(0, len(password1) - 1)
                password1[i1], password1[j1] = password1[j1], password1[i1]
        
        # Check for repetition of special characters and replace if necessary
        special_char_count = {char: password1.count(char) for char in special_chars1}
        for i1, char1 in enumerate(password1):
            if char1 in special_chars1 and special_char_count[char1] > 1:
                new_char = char1
                while new_char == char1 or new_char in used_special_chars:
                    new_char = r1.choice(special_chars1)
                password1[i1] = new_char
                special_char_count[char1] -= 1
                special_char_count[new_char] = special_char_count.get(new_char, 0) + 1
                used_special_chars.add(new_char)
        
        return ''.join(password1)
    
    passwords = [generate_password() for _ in range(num_passwords)]
    return [[p1] for p1 in passwords]

@xw.func
def RDSEEDMULTIPW(num_passwords):
    #return generate_passwords(num_passwords)
    return [[password] for password in generate_passwords(num_passwords)]


@xw.func
def RDSEED_MATRIX(min_val, max_val, count, rows, cols):
    count=int(count)
    rows=int(rows)
    cols=int(cols)
    random_numbers_rdseed = []
    for _ in range(count):
        random_number_rdseed = rdrand.RdSeedom().get_bits(64) % (max_val - min_val + 1) + min_val
        random_numbers_rdseed.append(random_number_rdseed)
    
    # Create a 2D array to hold the random numbers
    matrix = []
    index = 0
    for i in range(rows):
        row = []
        for j in range(cols):
            if index < len(random_numbers_rdseed):
                row.append(random_numbers_rdseed[index])
                index += 1
            else:
                row.append(None)  # Fill with None if there are not enough random numbers
        matrix.append(row)
    
    return matrix

# Function to generate passwords
def generate_passwords(num_passwords):
    try:
        # Convert the input to an integer
        num_passwords = int(num_passwords)
    except (ValueError, TypeError):
        return "Input must be a positive integer."

    # Validate the converted number
    if num_passwords <= 0:
        return "Input must be a positive integer."

    # Helper function to generate one password
    def generate_password():
        special_characters = "?@$#^&*"
        letters_and_digits = (
            string.ascii_uppercase + string.ascii_lowercase + string.digits
        )
        all_characters = letters_and_digits + special_characters

        # Instantiate the RdSeedom object
        seed = RdSeedom()

        while True:
            # Select required characters
            upper = seed.choice(string.ascii_uppercase)
            lower = seed.choice(string.ascii_lowercase)
            digit = seed.choice(string.digits)
            specials = seed.sample(special_characters, 2)  # Pick exactly 2 special characters

            # Remaining characters should come only from letters and digits
            others = seed.sample(letters_and_digits, 7)  # To ensure no extra special characters

            # Combine all characters
            password_list = [upper, lower, digit] + specials + others

            # Shuffle using RdSeedom
            for i in range(len(password_list) - 1, 0, -1):
                j = seed.randrange(0, i + 1)
                password_list[i], password_list[j] = password_list[j], password_list[i]

            password = ''.join(password_list)

            # Ensure it doesn't start with a special character
            if not password[0] in special_characters:
                return password

    # Generate the requested number of passwords
    passwords = [generate_password() for _ in range(num_passwords)]
    return passwords

# Excel UDF
@xw.func
def xl_generate_passwords(num_passwords):
    #return generate_passwords(num_passwords)
    return [[password] for password in generate_passwords(num_passwords)]

@xw.func
def PRIME(lower, upper):
    lower = int(lower)
    upper = int(upper)
    
    def is_prime(n):
        if n <= 1:
            return False
        for i in range(2, int(n**0.5) + 1):
            if n % i == 0:
                return False
        return True

    primes = [num for num in range(lower, upper + 1) if is_prime(num)]
    return [[prime] for prime in primes]

@xw.func
def IS_PRIME_WITH_DIVISOR(number):
    number = int(number)
    
    def is_prime(n):
        if n <= 1:
            return False, 1
        for i in range(2, int(n**0.5) + 1):
            if n % i == 0:
                return False, i
        return True, 1

    prime, divisor = is_prime(number)
    return ["Yes" if prime else "No", divisor]


@xw.func
def prime_factors(n):
    """Returns all prime factors (including duplicates)"""
    if isinstance(n, list):
        results = []
        max_length = 0
        for num in n:
            factors = _calculate_prime_factors(num)
            results.append(factors)
            if len(factors) > max_length:
                max_length = len(factors)
        
        # Pad each list to the max length
        for i in range(len(results)):
            results[i] += [None] * (max_length - len(results[i]))
        
        return results
    else:
        return _calculate_prime_factors(n)

def _calculate_prime_factors(n):
    if n <= 1:
        return [n]  # Return the number itself if it's <= 1
    
    # Get prime factors with duplicates
    prime_factors = []
    remaining = n
    i = 2
    while i * i <= remaining:
        while remaining % i == 0:
            prime_factors.append(i)
            remaining = remaining // i
        i += 1
    if remaining > 1:
        prime_factors.append(remaining)
    
    # Return prime factors without the highest prime factor
    return prime_factors

# Function to apply prime_factors to a range
def apply_prime_factors_to_range():
    wb = xw.Book.caller()  # Connect to the calling Excel workbook
    sheet = wb.sheets[0]  # Use the first sheet
    input_range = sheet.range('A1:A16')  # Adjust the range as needed
    output_range = sheet.range('B1:B16')  # Adjust the output range as needed
    
    for i, cell in enumerate(input_range):
        number = cell.value
        factors = prime_factors(number)
        output_range[i].value = factors

# Call the function when the script is run
if __name__ == "__main__":
    xw.Book("YourWorkbookName.xlsx").set_mock_caller()
    apply_prime_factors_to_range()


@xw.func
@xw.arg('data', ndim=1)
def SPLIT_TEXT_MODIFIED(data, delimiter):
    try:
        # Step 1: Filter out completely blank or whitespace-only cells
        filtered_data = [cell for cell in data if cell and cell.strip()]
        
        if not filtered_data:
            return [[""]]
        
        # Step 2: Split each row by the delimiter
        split_data = [cell.split(delimiter) for cell in filtered_data]
        
        # Step 3: Remove rows that are entirely empty after splitting
        cleaned_data = [row for row in split_data if any(cell.strip() for cell in row)]
        
        # Step 4: Pad rows to equal length
        max_length = max(len(row) for row in cleaned_data)
        padded_data = [row + [""] * (max_length - len(row)) for row in cleaned_data]
        
        return padded_data
    except Exception as e:
        return str(e)

@xw.func
def PYTHON_METHODS(source_range: xw.Range, operation: xw.Range, append_range: xw.Range = None, index: xw.Range=None, item: xw.Range=None):
    """
    Performs various Python list operations on source_range based on the operation specified.
    
    Args:
        source_range (xw.Range): The range containing the original values.
        operation (xw.Range): A single cell containing the operation to perform (e.g., ".append", ".extend", ".clear", etc.).
        append_range (xw.Range): The range containing the values to append (optional, used for ".append" and ".extend").
        index (xw.Range): A single cell containing the index for operations like ".insert" or ".pop" (optional).
        item (xw.Range): A single cell containing the item for operations like ".count", ".index", ".insert", or ".remove" (optional).
    
    Returns:
        list or str: The modified list or the result of the operation (e.g., count, index).
    """
    # Convert source_range to a list
    source_values = source_range.value if isinstance(source_range.value, list) else [source_range.value]
    source_values = [item for sublist in source_values for item in (sublist if isinstance(sublist, list) else [sublist])]

    # Handle operations
    if operation.value == ".append":
        if append_range is None:
            return "Error: append_range is required for '.append'."
        append_values = append_range.value if isinstance(append_range.value, list) else [append_range.value]
        for value in append_values:
            source_values.append(value)
        return source_values

    elif operation.value == ".extend":
        if append_range is None:
            return "Error: append_range is required for '.extend'."
        append_values = append_range.value if isinstance(append_range.value, list) else [append_range.value]
        for value in append_values:
            if isinstance(value, (list, tuple, str)):
                source_values.extend(value)
            else:
                source_values.extend([value])
        return source_values

    elif operation.value == ".clear":
        source_values.clear()
        return None

    elif operation.value == ".count":
        if item is None:
            return "Error: item is required for '.count'."
        return source_values.count(item.value)

    elif operation.value == ".copy":
        return source_values.copy()

    elif operation.value == ".index":
        if item is None:
            return "Error: item is required for '.index'."
        try:
            return source_values.index(item.value)
        except ValueError:
            return f"Error: {item.value} not found in the list."

    elif operation.value == ".insert":
        if index is None or item is None:
            return "Error: index and item are required for '.insert'."
        try:
            source_values.insert(int(index.value), item.value)
            return source_values
        except (ValueError, TypeError):
            return "Error: Invalid index."

    elif operation.value == ".pop":
        if index is None:
            try:
                return source_values.pop()
            except IndexError:
                return "Error: Cannot pop from an empty list."
        else:
            try:
                return source_values.pop(int(index.value))
            except (IndexError, ValueError, TypeError):
                return "Error: Invalid index."

    elif operation.value == ".remove":
        if item is None:
            return "Error: item is required for '.remove'."
        try:
            source_values.remove(item.value)
            return source_values
        except ValueError:
            return f"Error: {item.value} not found in the list."

    elif operation.value == ".reverse":
        source_values.reverse()
        return source_values

    else:
        return "Error: Unsupported operation. Supported operations are '.append', '.extend', '.clear', '.count', '.copy', '.index', '.insert', '.pop', '.remove', '.reverse'."

# ==================== FROM udf3-dll.py ====================

import os
import sys

os.environ['PATH'] = r'D:\Programs\msys64\ucrt64\bin' + ';' + os.environ['PATH']
os.add_dll_directory(r'D:\Programs\msys64\ucrt64\bin')

import cffi

# Setup cffi
ffi = cffi.FFI()
ffi.cdef("""
int match_pattern_in_array(char **input_array, int array_length, const char *pattern, char ***output_array);
void free_matches(char **matches, int match_count);
""")

dll = ffi.dlopen(r"D:\Programs\msys64\home\juhi\Downloads\regex-C-xlwings.dll")

@xw.func
def match_pattern_udf(input_list, pattern):
    try:
        # Convert input strings into a C array
        input_array = [ffi.new("char[]", (item or "").encode('utf-8')) for item in input_list]
        input_array_c = ffi.new("char*[]", input_array)
        
        # Prepare output array (pointer to pointer)
        output_array_c = ffi.new("const char***")
        
        # Call the DLL function
        match_count = dll.match_pattern_in_array(input_array_c, len(input_list), pattern.encode('utf-8'), output_array_c)
        
        # If the match count is negative, return an empty list
        if match_count < 0:
            return [[""] for _ in input_list]  # Return an empty list vertically
        
        # Convert output back to Python
        output_list = [ffi.string(output_array_c[0][i]).decode('utf-8') for i in range(match_count)]
        
        # Create a result list that matches the original input length
        final_output = [[item] if item != "" else [""] for item in output_list]  # Vertical output (column)
        
        # Free the output array allocated by the DLL
        dll.free_matches(output_array_c[0], match_count)
        
        return final_output
    except Exception as e:
        print(f"Error: {e}")
        return [[""] for _ in input_list]  # Return vertical empty list on error
# Add MSYS2 directories to PATH
os.environ['PATH'] = (
    r'D:\dev\dll;' +
    r'D:\Programs\msys64\ucrt64\bin;' +
    r'D:\Programs\msys64\ucrt64\lib;' +
    r'D:\Programs\msys64\ucrt64\include;' +
    r'D:\Programs\msys64\ucrt64\x86_64-w64-mingw32\bin;' +
    r'D:\Programs\msys64\ucrt64\x86_64-w64-mingw32\include;' +
    os.environ['PATH']
)
ffi201 = FFI()
#NUM_NUMBERS = 100000
#NUM_THREADS = 16

# Define the functions in the DLL
ffi201.cdef("""
    int rdrand64_step(unsigned long long *rand);
    void generate_random_numbers(int num_threads, int num_numbers);
    unsigned long long* get_numbers();
    void free_numbers(unsigned long long *numbers);
""")


# Load the DLL
#C = ffi.dlopen('D:\\OneDrive - 0yt2k\\Compiled dlls & executables\\rdrand_multithreaded_new_ucrt_gcc.dll')
C = ffi201.dlopen('rdrand_multithreaded_new.dll')
@xw.func
def generate_and_get_data(NUM_THREADS, NUM_NUMBERS):
    NUM_THREADS = int(NUM_THREADS)
    NUM_NUMBERS = int(NUM_NUMBERS)
    C.generate_random_numbers(NUM_THREADS, NUM_NUMBERS)
    numbers_ptr = C.get_numbers()
    numbers = [[int(numbers_ptr[i])] for i in range(NUM_NUMBERS)]
    C.free_numbers(numbers_ptr)
    return numbers

# ==================== FROM xl-to-js.py ====================
@xw.func
def compare_pre_increment(values):
    # Check if the input is a list of lists or a flat list
    if isinstance(values[0], list):
        # Flatten the list of lists into a single list
        flat_values = [item for sublist in values for item in sublist]
    else:
        # Use the flat list directly
        flat_values = values

    # JavaScript code for pre-increment
    js_code = """
    function preIncrement(values) {
        return values.map(function(value) {
            var originalValue = value;
            value++;
            return [originalValue, value];
        });
    }
    """
    
    # Use execjs to run the JavaScript code
    ctx = execjs.compile(js_code)
    js_result = ctx.call("preIncrement", flat_values)

    # Python pre-increment
    def pre_increment_py(values):
        return [value + 1 for value in values]

    py_result = pre_increment_py(flat_values)

    # Prepare output with headings
    output = [["preincrement-JS-Original", "preincrement-JS-Incremented", "preincrement-PY"]]

    # Append results side by side (JS original, JS incremented, PY incremented)
    for (js_orig, js_inc), py_val in zip(js_result, py_result):
        output.append([js_orig, js_inc, py_val])

    return output

@xw.func
def compare_non_strict_equivalent(values1, values2):
    # Function to replace None values with 'null' and ensure everything is a list of lists
    def replace_none_with_null(values):
        if isinstance(values, (int, float)):  # Single value, not a list
            return [['null' if values is None else values]]
        elif isinstance(values, list) and not isinstance(values[0], list):  # Single row
            return [['null' if val is None else val for val in values]]
        else:  # List of rows
            return [[('null' if val is None else val) for val in row] for row in values]

    # Replace None values with 'null' in both input arrays
    values1 = replace_none_with_null(values1)
    values2 = replace_none_with_null(values2)

    # JavaScript function to check non-strict equality
    js_code = """
    function CompareNonStrict(values1, values2) {
        return values1.map((row, i) => 
            row.map((value, j) => {
                return value == values2[i][j];
            })
        );
    }
    """

    # Use execjs to compile and run the JavaScript function
    ctx = execjs.compile(js_code)

    # Apply JavaScript non-strict equality check
    try:
        js_result = ctx.call("CompareNonStrict", values1, values2)
    except Exception as e:
        js_result = [[str(e)] * len(values1[0])] * len(values1)

    # Python equality check (loose)
    def py_equal(values1, values2):
        # Python loose comparison (None should be treated as 'null' for loose equality)
        return [[val1 == val2 for val1, val2 in zip(row1, row2)] for row1, row2 in zip(values1, values2)]

    try:
        py_result = py_equal(values1, values2)
    except Exception as e:
        py_result = [[str(e)] * len(values1[0])] * len(values1)

    # Prepare output with headings
    output = [["JS-Non-Strict-Eq.(==)", "PY-Equal(==)"]]

    # Append results side by side (JS non-strict equality and Python equality)
    for js_row, py_row in zip(js_result, py_result):
        for js_val, py_val in zip(js_row, py_row):
            output.append([js_val, py_val])

    return output


# ==================== FROM regex-msys-c.py ====================
# Add MSYS2 UCRT library path to the system PATH
msys2_ucrt_lib_path = r'D:\dev\dll;D:\Programs\msys64\ucrt64\bin;D:\Programs\msys64\ucrt64\lib'  # Adjust this path to your MSYS2 UCRT bin directory
os.environ['PATH'] = msys2_ucrt_lib_path + os.pathsep + os.environ['PATH']

# Define the full path to the DLL file (adjust as necessary)
dll_path = r'regex-c-msys-single-threading.dll'

# Initialize cffi
ffi = cffi.FFI()

# Define the C function signatures
ffi.cdef("""
    int match_patterns(const char** input_array, int array_length, const char* pattern, char*** output_array);
    void free_matches(char** matches, int match_count);
""")

# Load the compiled DLL
dll = ffi.dlopen(dll_path)

# Define a Python function to interact with the DLL
def match_patterns_s(input_array, pattern):
    # Convert Python strings to C-compatible strings (list of C char pointers)
    input_array_c = [ffi.new("char[]", s.encode('utf-8')) for s in input_array]  # Create a new cdata for each string
    input_array_ptrs = ffi.new("const char*[]", input_array_c)  # Create a pointer array

    # Prepare output for the matched patterns
    output_array_c = ffi.new("char***")
    
    # Call the C function from the DLL
    num_matches = dll.match_patterns(input_array_ptrs, len(input_array), pattern.encode('utf-8'), output_array_c)
    
    if num_matches == 0:
        return []
    
    # Convert the C output array back to a Python list of strings
    matches = [ffi.string(output_array_c[0][i]).decode('utf-8') for i in range(num_matches)]
    
    # Free the memory allocated by the DLL
    dll.free_matches(output_array_c[0], num_matches)
    
    return matches

# Define the Excel UDF
@xw.func
@xw.arg('input_array', ndim=1)
#@xw.ret(expand='vertical')
def REGEXMSYSC(input_array, pattern):
    # Convert Excel input to a list of Python strings
    input_list = [str(item) for item in input_array]
    
    # Call the match_patterns function and return the matches
    matches = match_patterns(input_list, pattern)
    
    return [[match] for match in matches]


# ==================== FROM regex-msys-c-multithreading.py ====================
# Add MSYS2 UCRT library path to the system PATH
msys2_ucrt_lib_path = r'D:\dev\dll;D:\Programs\msys64\ucrt64\bin;D:\Programs\msys64\ucrt64\lib'  # Adjust this path to your MSYS2 UCRT bin directory
os.environ['PATH'] = msys2_ucrt_lib_path + os.pathsep + os.environ['PATH']

# Define the full path to the multithreaded DLL file (adjust as necessary)
dll_path = r'regex-msys-c-pcre2-multithreading.dll'  # Updated DLL
# Initialize cffi

ffi = cffi.FFI()

# Define the C function signatures (same as single-threaded, no changes here)
ffi.cdef("""
    int match_patterns(const char** input_array, int array_length, const char* pattern, char*** output_array);
    void free_matches(char** matches, int match_count);
""")

# Load the compiled multithreaded DLL
dll = ffi.dlopen(dll_path)

# Define a Python function to interact with the DLL
def match_patterns(input_array, pattern):
    # Convert Python strings to C-compatible strings (list of C char pointers)
    input_array_c = [ffi.new("char[]", s.encode('utf-8')) for s in input_array]  # Create a new cdata for each string
    input_array_ptrs = ffi.new("const char*[]", input_array_c)  # Create a pointer array

    # Prepare output for the matched patterns
    output_array_c = ffi.new("char***")
    
    # Call the C function from the DLL (same as single-threaded code)
    num_matches = dll.match_patterns(input_array_ptrs, len(input_array), pattern.encode('utf-8'), output_array_c)
    
    if num_matches == 0:
        return []
    
    # Convert the C output array back to a Python list of strings
    matches = [ffi.string(output_array_c[0][i]).decode('utf-8') for i in range(num_matches)]
    
    # Free the memory allocated by the DLL
    dll.free_matches(output_array_c[0], num_matches)
    
    return matches

# Define the Excel UDF
@xw.func
@xw.arg('input_array', ndim=1)
#@xw.ret(expand='vertical')  # Ensures the results are outputted vertically in Excel
def REGEXMSYSC2(input_array, pattern):
    # Convert Excel input to a list of Python strings
    input_list = [str(item) for item in input_array]
    
    # Call the match_patterns function and return the matches
    matches = match_patterns(input_list, pattern)
    
    return [[match] for match in matches]  # Wrap each match in its own list to return a vertical array

ffi = FFI()

# Define the C function signature
ffi.cdef("void compare_custom(double* arr1, double* arr2, int length, bool* result);")

os.environ['PATH'] = r'D:\dev\dll;D:\Programs\msys64\ucrt64\bin;D:\Programs\msys64\ucrt64\lib'+ os.pathsep + os.environ['PATH']

# Load the shared DLL
lib = ffi.dlopen(r'customoperator.dll')

@xw.func
def compare_custom_operator(values):
    arr1 = [row[0] for row in values]
    arr2 = [row[1] for row in values]
    length = len(arr1)

    arr1_c = ffi.new("double[]", arr1)
    arr2_c = ffi.new("double[]", arr2)
    result_c = ffi.new("bool[]", length)

    # Call the DLL function
    lib.compare_custom(arr1_c, arr2_c, length, result_c)

    # Convert the C result to Python list
    result = [[bool(result_c[i])] for i in range(length)]
    return result


# ==================== FROM regex-boost-msys.py ====================
# Set environment variables for Boost and DLL paths
os.environ['PATH'] = r'D:\Programs\msys64\ucrt64\bin;D:\Programs\msys64\ucrt64\lib' + ';' + os.environ['PATH'] 

# Load the compiled DLL
dll = ctypes.CDLL(r'D:\dev\dll\regex-boost-msys-new.dll')

# Define the function signatures
dll.match_patterns.argtypes = [POINTER(c_char_p), c_int, c_char_p, POINTER(POINTER(c_char_p))]
dll.match_patterns.restype = c_int
dll.free_matches.argtypes = [POINTER(c_char_p), c_int]
dll.free_matches.restype = None

@xw.func
def REGEXMSYSB1(input_array, pattern):
    # Convert input array to ctypes array
    input_array_ctypes = (c_char_p * len(input_array))(*[s.encode('utf-8') for s in input_array])
    pattern_ctypes = pattern.encode('utf-8')
    
    # Prepare output array
    output_array_ctypes = POINTER(c_char_p)()
    
    # Call the DLL function
    num_matches = dll.match_patterns(input_array_ctypes, len(input_array), pattern_ctypes, byref(output_array_ctypes))
    
    # Convert output array to Python list
    output_array = [output_array_ctypes[i].decode('utf-8') for i in range(num_matches)]
    
    # Free the allocated memory
    dll.free_matches(output_array_ctypes, num_matches)
    
    vertical_output_array = [[item] for item in output_array]
    return vertical_output_array



# ==================== FROM intcalc.py ====================
def add_months(start_date, months):
    month = start_date.month - 1 + months
    year = start_date.year + month // 12
    month = month % 12 + 1
    day = min(start_date.day, [31,
        29 if year % 4 == 0 and not year % 100 == 0 or year % 400 == 0 else 28,
        31, 30, 31, 30, 31, 31, 30, 31, 30, 31][month-1])
    return start_date.replace(year=year, month=month, day=day)

@xw.func
def derive_annual_interest_rates(start_dates, end_dates, start_amounts, end_amounts):
    # Convert Excel serial date to datetime object
    def excel_date_to_datetime(excel_date):
        if isinstance(excel_date, (int, float)):
            return datetime(1899, 12, 30) + timedelta(days=int(excel_date))
        return excel_date
    
    # Function to calculate the number of days between two dates
    def days_between(d1, d2):
        return (d2 - d1).days
    
    # Function to calculate the end amount given a rate
    def calculate_end_amount(rate, start_date, end_date, start_amount):
        current_amount = start_amount
        current_date = start_date
        
        while current_date < end_date:
            if current_date == start_date:
                next_date = add_months(current_date, 3)  # Add three months for the first quarter
                days = days_between(current_date, next_date) + 1  # Add one day for the first quarter
            else:
                next_date = add_months(current_date, 3)  # Add three months for subsequent quarters
                if next_date > end_date:
                    next_date = end_date
                    days = days_between(current_date, next_date) - 1  # Subtract one day for the last quarter
                else:
                    days = days_between(current_date, next_date)
            
            interest = (rate / 100) * (days / 365) * current_amount
            current_amount += interest
            
            current_date = next_date
        
        return current_amount
    
    # Ensure inputs are lists
    if not isinstance(start_dates, list):
        start_dates = [start_dates]
    if not isinstance(end_dates, list):
        end_dates = [end_dates]
    if not isinstance(start_amounts, list):
        start_amounts = [start_amounts]
    if not isinstance(end_amounts, list):
        end_amounts = [end_amounts]
    
    derived_rates = []
    
    for i in range(len(start_dates)):
        start_date = excel_date_to_datetime(start_dates[i])
        end_date = excel_date_to_datetime(end_dates[i])
        start_amount = start_amounts[i]
        end_amount = end_amounts[i]
        
        # Initialize variables
        low_rate = 0.0
        high_rate = 50.0  # Remove the upper limit for the interest rate
        tolerance = 1e-10  # Tolerance for convergence
        max_iterations = 1000  # Maximum number of iterations to prevent infinite loop
        
        # Binary search for the correct rate
        for _ in range(max_iterations):
            mid_rate = (low_rate + high_rate) / 2
            calculated_end_amount = calculate_end_amount(mid_rate, start_date, end_date, start_amount)
            
            if abs(calculated_end_amount - end_amount) < tolerance:
                break
            
            if calculated_end_amount < end_amount:
                low_rate = mid_rate
            else:
                high_rate = mid_rate
        
        derived_rate = (low_rate + high_rate) / 2
        derived_rates.append(derived_rate)
    
    return [[rate] for rate in derived_rates]


# Example usage in Excel:
# Assuming you have ranges A1:A10 for start_dates, B1:B10 for end_dates,
# C1:C10 for start_amounts, and D1:D10 for end_amounts,
# you can use the function in Excel as follows:
# =derive_annual_interest_rates(A1:A10, B1:B10, C1:C10, D1:D10)

def add_months1(start_date1, months1):
    month1 = start_date1.month - 1 + months1
    year1 = start_date1.year + month1 // 12
    month1 = month1 % 12 + 1
    day1 = min(start_date1.day, [31,
        29 if year1 % 4 == 0 and not year1 % 100 == 0 or year1 % 400 == 0 else 28,
        31, 30, 31, 30, 31, 31, 30, 31, 30, 31][month1-1])
    return start_date1.replace(year=year1, month=month1, day=day1)
@xw.func
def SIMPLERATEDERIVATION(start_dates1, end_dates1, start_amounts1, end_amounts1):
    # Convert Excel serial date to datetime object
    def excel_date_to_datetime1(excel_date1):
        if isinstance(excel_date1, (int, float)):
            return datetime(1899, 12, 30) + timedelta(days=int(excel_date1))
        return excel_date1
    # Function to calculate the number of days between two dates
    def days_between1(d3, d4):
        return (d4 - d3).days
    # Function to calculate the end amount given a rate
    def calculate_end_amount1(rate1, start_date1, end_date1, start_amount1):
        current_amount1 = start_amount1
        current_date1 = start_date1
        while current_date1 < end_date1:
            if current_date1 == start_date1:
                next_date1 = add_months1(current_date1, 3)  # Add three months for the first quarter
                days1 = days_between1(current_date1, next_date1)  # Add one day for the first quarter
            else:
                next_date1 = add_months1(current_date1, 3)  # Add three months for subsequent quarters
                if next_date1 > end_date1:
                    next_date1 = end_date1
                    days1 = days_between1(current_date1, next_date1)  # Subtract one day for the last quarter
                else:
                    days1 = days_between1(current_date1, next_date1)
            interest1 = (rate1 / 100) * (days1 / 365) * current_amount1
            current_amount1 += interest1
            current_date1 = next_date1
        return current_amount1
    # Ensure inputs are lists
    if not isinstance(start_dates1, list):
        start_dates1 = [start_dates1]
    if not isinstance(end_dates1, list):
        end_dates1 = [end_dates1]
    if not isinstance(start_amounts1, list):
        start_amounts1 = [start_amounts1]
    if not isinstance(end_amounts1, list):
        end_amounts1 = [end_amounts1]
    derived_rates1 = []
    for i in range(len(start_dates1)):
        start_date1 = excel_date_to_datetime1(start_dates1[i])
        end_date1 = excel_date_to_datetime1(end_dates1[i])
        start_amount1 = start_amounts1[i]
        end_amount1 = end_amounts1[i]
        # Initialize variables
        low_rate1 = 0.0
        high_rate1 = 50.0  # Remove the upper limit for the interest rate
        tolerance1 = 1e-10  # Tolerance for convergence
        max_iterations1 = 1000  # Maximum number of iterations to prevent infinite loop
        # Binary search for the correct rate
        for _ in range(max_iterations1):
            mid_rate1 = (low_rate1 + high_rate1) / 2
            calculated_end_amount1 = calculate_end_amount1(mid_rate1, start_date1, end_date1, start_amount1)
            if abs(calculated_end_amount1 - end_amount1) < tolerance1:
                break
            if calculated_end_amount1 < end_amount1:
                low_rate1 = mid_rate1
            else:
                high_rate1 = mid_rate1
        derived_rate1 = (low_rate1 + high_rate1) / 2
        derived_rates1.append(derived_rate1)
    return [[rate1] for rate1 in derived_rates1]

# ==================== FROM xl_to_js2.py ====================
@xw.func
def js_compare_pre_increment(values):
    # Ensure values is always a list of lists
    if not isinstance(values, list):
        values = [[values]]
    elif not isinstance(values[0], list):
        values = [[v] for v in values]

    # Flatten the list of lists into a single list
    flat_values = [item if item is not None else 'null' for sublist in values for item in sublist]

    # JavaScript code for pre-increment
    js_code_pre_increment = """
    function preIncrement(values) {
        return values.map(function(value) {
            if (value === 'null') {
                return ['null', 'null'];
            }
            var originalValue = parseFloat(value);
            value = originalValue + 1;
            return [originalValue, value];
        });
    }
    """
    
    # Use JSPyBridge to run the JavaScript code
    vm = require('vm')
    ctx = vm.createContext({})
    vm.runInContext(js_code_pre_increment, ctx)

    # Run the JavaScript function
    js_result_pre_increment = ctx.preIncrement(flat_values)

    # Python pre-increment
    def pre_increment_py(values):
        return [float(value) + 1 if value != 'null' else 'null' for value in values]

    py_result = pre_increment_py(flat_values)

    # Prepare output with headings
    output = [["preincrement-JS-Original", "preincrement-JS-Incremented", "preincrement-PY"]]

    # Append results side by side (JS original, JS incremented, PY incremented)
    for (js_orig, js_inc), py_val in zip(js_result_pre_increment, py_result):
        output.append([js_orig, js_inc, py_val])

    return output

@xw.func
def js_compare_strict_equivalent(values):
    # Ensure values is always a list of lists
    if not isinstance(values, list):
        values = [[values]]
    elif not isinstance(values[0], list):
        values = [values]

    # Break down the input into two arrays
    Ar1 = [row[0] for row in values]
    Ar2 = [row[1] for row in values]

    # Function to replace None values with 'null'
    def replace_none_with_null(values):
        return ['null' if val is None else val for val in values]

    # Replace None values with 'null' in both arrays
    Ar1 = replace_none_with_null(Ar1)
    Ar2 = replace_none_with_null(Ar2)

    # JavaScript code for strict equality check
    js_code_strict = """
    function CompareStrict(values1, values2) {
        return values1.map((value, i) => {
            if (value === 'null' || values2[i] === 'null') {
                return value === values2[i];
            }
            return value === values2[i];
        });
    }
    """

    # Use JSPyBridge to run the JavaScript code
    vm = require('vm')

    # Create a new context and run the JavaScript code
    ctx = vm.createContext({})
    vm.runInContext(js_code_strict, ctx)

    # Run the JavaScript function
    js_result_strict = ctx.CompareStrict(Ar1, Ar2)

    # Python equality check (loose)
    def py_equal(values1, values2):
        return [val1 == val2 for val1, val2 in zip(values1, values2)]

    py_result_non_strict = py_equal(Ar1, Ar2)

    # Prepare output with headings
    output = [["JS-Strict-Eq.(===)", "PY-Equal(==)"]]

    # Append results side by side (JS strict equality, Python equality)
    for js_res, py_res in zip(js_result_strict, py_result_non_strict):
        output.append([js_res, py_res])

    return output

@xw.func
def js_compare_non_strict_equivalent(values):
    # Ensure values is always a list of lists
    if not isinstance(values, list):
        values = [[values]]
    elif not isinstance(values[0], list):
        values = [values]

    # Break down the input into two arrays
    Ar1 = [row[0] for row in values]
    Ar2 = [row[1] for row in values]

    # Function to replace None values with 'null'
    def replace_none_with_null(values):
        return ['null' if val is None else val for val in values]

    # Replace None values with 'null' in both arrays
    Ar1 = replace_none_with_null(Ar1)
    Ar2 = replace_none_with_null(Ar2)

    # JavaScript code for non-strict equality check
    js_code_non_strict = """
    function CompareNonStrict(values1, values2) {
        return values1.map((value, i) => {
            if (value === 'null' || values2[i] === 'null') {
                return value == values2[i];
            }
            return value == values2[i];
        });
    }
    """

    # Use JSPyBridge to run the JavaScript code
    vm = require('vm')

    # Create a new context and run the JavaScript code
    ctx = vm.createContext({})
    vm.runInContext(js_code_non_strict, ctx)

    # Run the JavaScript function
    js_result_non_strict = ctx.CompareNonStrict(Ar1, Ar2)

    # Python equality check (loose)
    def py_equal(values1, values2):
        return [val1 == val2 for val1, val2 in zip(values1, values2)]

    py_result_non_strict = py_equal(Ar1, Ar2)

    # Prepare output with headings
    output = [["JS-Non-Strict-Eq.(==)", "PY-Equal(==)"]]

    # Append results side by side (JS non-strict equality, Python equality)
    for js_res, py_res in zip(js_result_non_strict, py_result_non_strict):
        output.append([js_res, py_res])

    return output


# ==================== FROM regex-msys-dll.py ====================
# Define the C function signatures and load the DLL
ffi = cffi.FFI()
ffi.cdef("""
    int match_patterns(const char **input_array, int array_length, const char *pattern, char ***output_array);
    void free_matches(char **matches_array, int match_count);
""")

# Load the new DLL (adjust the path to where your new DLL is located)
os.environ['PATH'] = r'D:/dev/dll' + ';' + os.environ['PATH'] + ';' + r'D:\Programs\msys64\ucrt64\bin' + ';' + r'D:\Programs\msys64\ucrt64\lib'

dll = ffi.dlopen("regex-boost-msys.dll")

@xw.func
def REGEXVC(input_list, pattern):
    try:
        # Convert input strings into a C array
        input_array = [ffi.new("char[]", (item or "").encode('utf-8')) for item in input_list]
        input_array_c = ffi.new("char*[]", input_array)
        
        # Prepare output array (pointer to pointer for output)
        output_array_c = ffi.new("char***")
        
        # Call the DLL function
        match_count = dll.match_patterns(input_array_c, len(input_list), pattern.encode('utf-8'), output_array_c)
        
        # If the match count is negative, return an empty string ("" for no match)
        if match_count < 0:
            return ["" for _ in input_list]  # Return an empty list
        
        # Convert output back to Python
        output_list = [ffi.string(output_array_c[0][i]).decode('utf-8') for i in range(match_count)]
        
        # Create a result list matching the original input length
        final_output = [item if item in output_list else "" for item in input_list]
        
        # Free the output array allocated by the DLL
        dll.free_matches(output_array_c[0], match_count)
        # Return a vertical array by making each element a separate list (for vertical orientation)
        return [[item] for item in final_output]
    
    except Exception as e:
        # Handle any errors gracefully by returning a list of empty strings
        print(f"Error: {e}")
        return ["" for _ in input_list]


# ==================== FROM sqlite.py ====================
@xw.func
def sql_query_direct(query, *data_ranges):
    """
    Execute a SQL query on multiple Excel data ranges and return the results.
    Data ranges will be in the format 'A6:K2000'.

    Parameters:
        query (str): SQL query string to execute.
        *data_ranges (str): Excel ranges (e.g., "A6:K2000", "M6:W2000").

    Returns:
        list: Query results as a list of lists (for Excel output).
    """
    # Initialize an in-memory SQLite database
    conn = sqlite3.connect(":memory:")
    cursor = conn.cursor()

    # Read data from each range and load into pandas DataFrame
    dataframes = []

    # Loop over each data range
    for idx, data_range in enumerate(data_ranges):
        # Get the sheet and range from xlwings
        sheet = xw.Book.caller().sheets.active
        # Get the data from the Excel range (excluding headers)
        data = sheet.range(data_range).value
        
        if not data:
            continue
        
        # First row is headers, so we separate it
        headers = data[0]
        rows = data[1:]  # Remaining rows after header

        # Convert to a pandas DataFrame
        df = pd.DataFrame(rows, columns=headers)

        # Create a table for this DataFrame in SQLite
        table_name = f"a{idx + 1}"  # Dynamic table names: a1, a2, etc.
        df.to_sql(table_name, conn, if_exists='replace', index=False)

    # Execute the provided SQL query
    query_results = pd.read_sql_query(query, conn)

    # Convert the query results to a list of lists (including headers) to return to Excel
    result_list = [list(query_results.columns)] + query_results.values.tolist()

    # Close the SQLite connection
    conn.close()

    # Return the query results as a list of lists
    return result_list


# ==================== FROM xlwings_sqlite_python.py ====================
@xw.func
def process_data(data):
    IDS = ['26909']
    
    # Convert the input data (list of lists) to a pandas DataFrame, using the first row as headers
    df = pd.DataFrame(data[1:], columns=data[0])

    final_results = []

    # Process each ID
    for ID1 in IDS:
        filtered_df = df[(df['ID'] == ID1) & (df['Type'] != 'Purchase Order') & (df['Type'] != 'Sales Order')] #& (df['Date'] < '2024-07-01')]
        grouped_df = filtered_df.groupby(['Type', 'Project: ID', 'Account']).agg({'Amount': lambda x: x.sum() * -1}).reset_index()
        grouped_df.rename(columns={'Amount': 'Amt'}, inplace=True)
        final_results.append(grouped_df)

    # Combine all results into a single DataFrame
    combined_results = pd.concat(final_results, ignore_index=True)

    # Sort the combined results by 'Project: ID' in ascending order
    combined_results.sort_values(by=['Project: ID', 'Amt'], inplace=True)

    # Add headers to the output data
    output_data = [combined_results.columns.tolist()] + combined_results.values.tolist()

    # Return the combined results with headers as a dynamic array
    return output_data


@xw.func
@xw.arg('excel_range', ndim=2)
@xw.arg('patterns', ndim=1)
def REGEXSTR(excel_range, patterns):
    result = []
    for row in excel_range:
        row_result = []
        for cell in row:
            cell_str = str(cell)  # Convert cell to string
            cell_result = cell_str
            for pattern in patterns:
                lines = cell_result.splitlines()
                maxlen = len(lines)  # Get the number of lines
                if pattern == "Country":
                    cell_result = lines[maxlen - 1].strip() if maxlen > 0 else ""
                else:
                    cell_result = lines[maxlen - 2].strip() if maxlen > 1 else cell_result
                    match = re.search(pattern, cell_result, flags=re.UNICODE | re.DOTALL)
                    if match:
                        cell_result = match.group().strip()  # Update cell_result with the matched string
                    else:
                        cell_result = ""
                        break  # If any pattern does not match, break the loop
            row_result.append(cell_result)
        result.append(row_result)
    return result

@xw.func
def SPLIT_TEXT2(data, delimiter):
    try:
        if not data or all(cell is None for cell in data):
            return [""]
        # Split the text and find the maximum length of the sublists
        split_data = [cell.split(delimiter) if cell is not None else [""] for cell in data]
        max_length = max(len(sublist) for sublist in split_data)
        # Ensure all sublists have the same length by padding with empty strings
        padded_data = [sublist + [""] * (max_length - len(sublist)) for sublist in split_data]
        return padded_data
    except Exception as e:
        return str(e)


@xw.func
@xw.arg('excel_range', ndim=2)
@xw.arg('patterns', ndim=1)
def REGEXREPLM2(excel_range, patterns, replacement):
    # Input validation
    if not excel_range or not patterns:
        return [[""]]
    
    # Ensure replacement is a string
    replacement = str(replacement) if replacement is not None else ""

    # Convert Excel range to list of lists and handle None values
    data_array = [
        [str(cell).strip() if cell is not None else "" for cell in row]
        for row in excel_range
    ]

    # Convert to pandas DataFrame with string dtype
    df = pd.DataFrame(data_array).astype("string")

    # Convert patterns to list if it's a single string
    if isinstance(patterns, str):
        patterns = [patterns]

    # Apply regex replacements
    for pattern in patterns:
        if pattern:
            pattern = str(pattern)
            df = df.replace(pattern, replacement, regex=True)

    # Convert back to nested list
    result = df.values.tolist()

    # Final cleanup of any remaining None or 'None' strings
    result = [
    ["" if cell is None else str(cell).strip() 
     for cell in row]
    for row in result
]

    return result

@xw.func
def testfunction(rng):
    return [[x*2] for x in rng]


@xw.func
@xw.arg('rng', 'range')
def testfunction3(rng, rf, cf):
    try:
        rf = int(rf)
        cf = int(cf)
        # Use the offset method to get the desired range
        offset_range = rng.offset(rf, cf)
        # Return the address of the offset range
        return offset_range.address
    except Exception as e:
        return str(e)

# ==================== FROM xlwings_sqlite_python_pandas.py ====================
@xw.func
def process_pandas(file_path):
    IDS = [32018, 30284, 30355, 31991, 32229, 32162, 32029, 29681, 30234, 28354, 31331, 32179, 29515, 30759, 31313, 30362, 29072, 29708, 30457, 30338, 29026, 30026, 27356, 29213, 28697, 13306, 12152]
    IDS = list(map(str,IDS))
    # Read the CSV file into a pandas DataFrame with headers and quotes set to true
    df = pd.read_csv(file_path, header=0, quotechar='"')
    df.columns = df.columns.str.strip()
    df['Project: ID'] = df['Project: ID'].fillna('')

    # Create the 'ID' column using vectorized regex operations
    id_pattern = r'(^\d+(?=\s-)|^\d+_\d+(?=\s-))'
    df['ID'] = df['Project: ID'].str.extract(id_pattern, expand=False).fillna('')

    # Sanitize 'Amount' and 'Qty' columns
    df['Amount'] = df['Amount'].str.replace(r',', '', regex=True).str.replace(r'\$', '', regex=True).str.replace(r'\(', '-', regex=True).str.replace(r'\)', '', regex=True)
    df['Amount'] = pd.to_numeric(df['Amount'], errors='coerce')

    df['Qty'] = df['Qty'].astype(str).str.replace(r',', '', regex=True).str.replace(r'\$', '', regex=True).str.replace(r'\(', '-', regex=True).str.replace(r'\)', '', regex=True)
    df['Qty'] = pd.to_numeric(df['Qty'], errors='coerce')

    # Convert 'Date' column to datetime
    df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y')

    final_results = []

    # Process each ID
    for ID1 in IDS:
        filtered_df = df[(df['ID'] == ID1) & (df['Type'] != 'Purchase Order') & (df['Type'] != 'Sales Order') & (~df['Account'].str.contains(r'(?i)advances', na=False)) & (~df['Account'].str.contains(r'(?i)payable', na=False))] #& (df['Date'] < pd.Timestamp('2024-07-01'))]
        grouped_df = filtered_df.groupby(['Type', 'Project: ID', 'Account']).agg({'Amount': 'sum'}).reset_index()
        grouped_df['Amt'] = -grouped_df['Amount'].where(grouped_df['Amount'] > 0, grouped_df['Amount'])
        grouped_df.drop(columns=['Amount'], inplace=True)
        final_results.append(grouped_df)

    # Combine all results into a single DataFrame
    combined_results = pd.concat(final_results, ignore_index=True)

    # Sort the combined results by 'Project: ID' in ascending order
    combined_results.sort_values(by=['Project: ID', 'Amt'], inplace=True)

    # Add headers to the output data
    output_data = [combined_results.columns.tolist()] + combined_results.values.tolist()

    # Return the combined results with headers as a dynamic array
    return output_data

#import pandas as pd
#import xlwings as xw
#import numpy as np
#from numba import jit

#@jit(nopython=True)
#def sanitize_amount_qty(amounts, qtys):
#    sanitized_amounts = np.empty(len(amounts), dtype=np.float64)
#    sanitized_qtys = np.empty(len(qtys), dtype=np.float64)
    
#    for i in range(len(amounts)):
#        sanitized_amounts[i] = amounts[i]
#        sanitized_qtys[i] = qtys[i]
    
#    return sanitized_amounts, sanitized_qtys

#@xw.func
#def process_pandas_numba(file_path):
#    IDS = ['32075']
#    # Read the CSV file into a pandas DataFrame with headers and quotes set to true
#    df = pd.read_csv(file_path, header=0, quotechar='"')
#    df.columns = df.columns.str.strip()
#    df['Project: ID'] = df['Project: ID'].fillna('')

    # Create the 'ID' column using vectorized regex operations
#    id_pattern = r'(^\d+(?=\s-)|^\d+_\d+(?=\s-))'
#    df['ID'] = df['Project: ID'].str.extract(id_pattern, expand=False).fillna('')

    # Sanitize 'Amount' and 'Qty' columns
#    df['Amount'] = df['Amount'].str.replace(r',', '', regex=True).str.replace(r'\$', '', regex=True).str.replace(r'\(', '-', regex=True).str.replace(r'\)', '', regex=True)
#    df['Amount'] = pd.to_numeric(df['Amount'], errors='coerce')

#    df['Qty'] = df['Qty'].astype(str).str.replace(r',', '', regex=True).str.replace(r'\$', '', regex=True).str.replace(r'\(', '-', regex=True).str.replace(r'\)', '', regex=True)
#    df['Qty'] = pd.to_numeric(df['Qty'], errors='coerce')

    # Convert 'Amount' and 'Qty' columns to numpy arrays
#    amounts = df['Amount'].values
#    qtys = df['Qty'].values

    # Sanitize 'Amount' and 'Qty' columns using Numba
#    sanitized_amounts, sanitized_qtys = sanitize_amount_qty(amounts, qtys)
#    df['Amount'] = sanitized_amounts
#    df['Qty'] = sanitized_qtys

    # Convert 'Date' column to datetime
#    df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y')

#    final_results = []

    # Process each ID
#    for ID1 in IDS:
#        filtered_df = df[(df['ID'] == ID1) & (df['Type'] != 'Purchase Order') & (df['Type'] != 'Sales Order')] #& (df['Date'] < pd.Timestamp('2024-07-01'))]
#        grouped_df = filtered_df.groupby(['Type', 'Project: ID', 'Account']).agg({'Amount': 'sum'}).reset_index()
#        grouped_df['Amt'] = -grouped_df['Amount'].where(grouped_df['Amount'] > 0, grouped_df['Amount'])
#        grouped_df.drop(columns=['Amount'], inplace=True)
#        final_results.append(grouped_df)

    # Combine all results into a single DataFrame
#    combined_results = pd.concat(final_results, ignore_index=True)

    # Sort the combined results by 'Project: ID' in ascending order
#    combined_results.sort_values(by=['Project: ID', 'Amt'], inplace=True)

    # Add headers to the output data
#    output_data = [combined_results.columns.tolist()] + combined_results.values.tolist()

    # Return the combined results with headers as a dynamic array
#    return output_data
#    return df

@xw.func
def Project_Billings(file_path):
#    IDS = [32018, 30284, 30355, 31991, 32229, 32162, 32029, 29681, 30234, 28354, 31331, 32179, 29515, 30759, 31313, 30362, 29072, 29708, 30457, 30338, 29026, 30026, 27356, 29213, 28697, 13306, 12152]
#    IDS = list(map(str,IDS))
    # Read the CSV file into a pandas DataFrame with headers and quotes set to true
    df = pd.read_csv(file_path, header=0, quotechar='"')
    df.columns = df.columns.str.strip()
    df['ProjectSOPO'] = df['ProjectSOPO'].fillna('')

    # Create the 'ID' column using vectorized regex operations
    id_pattern = r'(^\d+(?=\s-)|^\d+_\d+(?=\s-))'
    df['ID'] = df['ProjectSOPO'].str.extract(id_pattern, expand=False).fillna('')

    # Sanitize 'Amount' and 'Qty' columns
    df[['Amount', 'Amount Due']] = df[['Amount', 'Amount Due']].replace({r',': '', r'\$': '', r'\(': '-', r'\)': ''}, regex=True)
    df[['Amount', 'Amount Due']] = pd.to_numeric(df[['Amount', 'Amount Due']].stack(), errors='coerce').unstack()
    
#    df['Qty'] = df['Qty'].astype(str).str.replace(r',', '', regex=True).str.replace(r'\$', '', regex=True).str.replace(r'\(', '-', regex=True).str.replace(r'\)', '', regex=True)
#    df['Qty'] = pd.to_numeric(df['Qty'], errors='coerce')

    # Convert 'Date' column to datetime
    df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y')

    final_results = []

    # Process each ID
#    for ID1 in IDS:
    filtered_df = df[df['Date Due'] != 'Paid']
    filtered_df = df[df['Date Due'] != 'Paid'].copy()  # Add .copy() to avoid SettingWithCopyWarning
    filtered_df.loc[:, 'Date Due'] = pd.to_datetime(filtered_df['Date Due'], format='%m/%d/%Y')
    #filtered_df['Date Due'] = pd.to_datetime(filtered_df['Date Due'], format='%m/%d/%Y')
    grouped_df = filtered_df.groupby(['ProjectSOPO', 'Account', 'Type', 'Document Number', 'Date', 'Date Due', 'Amount Due']).agg({'Amount': 'sum'}).reset_index()
    #grouped_df['Amt'] = grouped_df['Amount'].where(grouped_df['Amount'] > 0, grouped_df['Amount'])
    grouped_df['Amt'] = grouped_df['Amount']
    grouped_df.drop(columns=['Amount'], inplace=True)
    grouped_df = grouped_df[grouped_df['ProjectSOPO'].str.strip().astype(bool)]
    final_results.append(grouped_df)

    # Combine all results into a single DataFrame
    combined_results = pd.concat(final_results, ignore_index=True)

    # Sort the combined results by 'Project: ID' in ascending order
    combined_results.sort_values(by=['ProjectSOPO', 'Type', 'Document Number', 'Date', 'Amt'], inplace=True)
    combined_results = combined_results[combined_results['Amount Due'] != 0]
    combined_results = combined_results[['ProjectSOPO', 'Account', 'Type', 'Document Number', 'Date', 'Date Due', 'Amt', 'Amount Due']]


    # Add headers to the output data
    output_data = [combined_results.columns.tolist()] + combined_results.values.tolist()

    # Return the combined results with headers as a dynamic array
    return output_data

@xw.func
@xw.arg('excel_range', ndim=2)  # Expecting a 2D Excel range
@xw.arg('patterns', ndim=1)    # Expecting a 1D list of patterns
def REGEXSTR2(excel_range, patterns):
    result = []
    for row in excel_range:
        row_result = []
        for cell in row:
            cell_str = str(cell)  # Convert the cell content to string
            cell_result = []
            for pattern in patterns:
                try:
                    # Use regex module for variable-length lookbehind
                    match = re1.search(pattern, cell_str, flags=re.UNICODE)
                    if match:
                        cell_result.append(match.group(2))  # Add the matched string
                except re.error as e:
                    # Handle invalid regex pattern errors
                    return f"Regex Error: {e}"
            # Join matches only if all patterns are matched, otherwise return empty
            if len(cell_result) == len(patterns):
                row_result.append(" ".join(cell_result))
            else:
                row_result.append("")
        result.append(row_result)
    return result


@xw.func
def fill_project_and_so_amount(data):
    # Convert the input data to a pandas DataFrame, treating empty cells as NaN
    df = pd.DataFrame(data[1:], columns=data[0]).replace('', pd.NA)
    
    # Fill the missing 'ProjectSOPO' values with the previous non-missing value
    df['ProjectSOPO'] = df['ProjectSOPO'].ffill()
    
    # Fill the 'SO Amount' values only if the project name is the same as the row above and 'SO Amount' is missing
    mask = (df['ProjectSOPO'] == df['ProjectSOPO'].shift(1)) & (df['SO Amount'].isna())
    df.loc[mask, 'SO Amount'] = df['SO Amount'].shift(1)
    
    # Include headers in the output
    result = [data[0]] + df.values.tolist()
    
    return result


def convert_datetime_columns(df, datetime_columns):
    for col in datetime_columns:
        df[col] = pd.to_datetime(df[col], errors='coerce').dt.floor('us')
    return df


@xw.func
def Project_SO_Inv_Summary(data):
    df = pd.DataFrame(data[1:], columns=data[0]).replace('', pd.NA)
    df = convert_datetime_columns(df, ['Date'])  # Add your datetime columns here
    df_filtered = df[(df['Type'] == 'Invoice')]
    df_grouped = df_filtered.groupby(['ProjectSOPO', 'SO Amount'], dropna=False).agg({
        'Amount (Gross)': 'sum',
        'Date': 'max'
    }).reset_index()
    df_grouped.columns = ['ProjectSOPO', 'SO Amount', 'Invoice Amt', 'Last_Inv_Date']
    df_grouped['Last_Inv_Date'] = pd.to_datetime(df_grouped['Last_Inv_Date'])
    result = [df_grouped.columns.tolist()] + df_grouped.values.tolist()
    return result

@xw.func
def Customer_Invoice_Summary(data):
    df = pd.DataFrame(data[1:], columns=data[0]).replace('', pd.NA)
    df = convert_datetime_columns(df, ['Date'])  # Add your datetime columns here
    df['ProjectSOPO'] = df['ProjectSOPO'].ffill()
    df_invoice = df[df['Type'] == 'Invoice'].groupby(['Customer', 'ProjectSOPO'], dropna=False).agg({
        'Amount (Gross)': 'sum',
        'Date': 'max'
    }).reset_index()
    df_invoice.columns = ['Customer', 'ProjectSOPO', 'Invoice Amt', 'Last_Inv_Date']
    df_received = df[df['Type'] != 'Invoice'].groupby(['Customer', 'ProjectSOPO'], dropna=False).agg({
        'Amount (Gross)': lambda x: -x.sum(),
        'Date': 'max'
    }).reset_index()
    df_received.columns = ['Customer', 'ProjectSOPO', 'Received Amt', 'Last_Rec_Date']
    df_summary = pd.merge(df_invoice, df_received, on=['Customer', 'ProjectSOPO'], how='outer')
    result = [df_summary.columns.tolist()] + df_summary.values.tolist()
    return result

@xw.func
def Project_Invoice_Summary(data):
    df = pd.DataFrame(data[1:], columns=data[0]).replace('', pd.NA)
    df = convert_datetime_columns(df, ['Last_Inv_Date', 'Last_Rec_Date'])  # Add your datetime columns here
    df = df.assign(
        **{
            'Invoice Amt': df['Invoice Amt'].astype(float),
            'Received Amt': df['Received Amt'].astype(float),
            'Last_Inv_Date': pd.to_datetime(df['Last_Inv_Date']),
            'Last_Rec_Date': pd.to_datetime(df['Last_Rec_Date'])
        }
    )
    df_grouped = df.groupby(['ProjectSOPO'], dropna=False).agg({
        'Invoice Amt': 'sum',
        'Received Amt': 'sum',
        'Last_Inv_Date': 'max',
        'Last_Rec_Date': 'max'
    }).reset_index()
    df_grouped = df_grouped[(df_grouped['Last_Rec_Date'] >= pd.to_datetime('2025-01-01')) & (df_grouped['Last_Rec_Date'] <= pd.to_datetime('2025-01-31'))]
    df_grouped['Difference'] = df_grouped['Invoice Amt'] - df_grouped['Received Amt']
    df_grouped = df_grouped[['ProjectSOPO', 'Invoice Amt', 'Received Amt', 'Difference', 'Last_Inv_Date', 'Last_Rec_Date']]
    df_grouped.columns = ['ProjectSOPO', 'INV_AMT', 'RCT_AMT', 'Difference', 'Last_Inv_Date', 'Last_Rct_Date']
    result = [df_grouped.columns.tolist()] + df_grouped.values.tolist()
    return result

@xw.func
@xw.arg('data_range', xw.Range)
@xw.arg('start_date', (str, datetime), default='2024-01-01')
@xw.arg('end_date', (str, datetime), default='2024-12-31')
def get_project_class(data_range, start_date='2024-01-01', end_date='2024-12-31'):
    """
    UDF to process data similar to the SQL query without using pandas
    Args:
        data_range: Input range containing the data (must include Type, ProjectSOPO, Date, and Class: Name columns)
        start_date: Start date for filtering (default: '2024-01-01')
        end_date: End date for filtering (default: '2024-12-31')
    Returns:
        List of lists with ProjectSOPO and Class: Name columns
    """
    try:
        # Convert the data range to a list of lists
        data = data_range.value

        # Extract headers and data rows
        headers = data[0]
        rows = data[1:]

        # Get column indices
        type_idx = headers.index('Type')
        project_sopo_idx = headers.index('ProjectSOPO')
        date_idx = headers.index('Date')
        class_name_idx = headers.index('Class: Name')

        # Convert start and end dates to datetime objects if they are strings
        if isinstance(start_date, str):
            start_date = datetime.strptime(start_date, '%Y-%m-%d')
        if isinstance(end_date, str):
            end_date = datetime.strptime(end_date, '%Y-%m-%d')

        # Initialize a dictionary to store the results
        result_dict = {}

        # Iterate over the rows to filter and group data
        for row in rows:
            row_type = row[type_idx]
            row_project_sopo = row[project_sopo_idx] if row[project_sopo_idx] else 'Unknown'
            row_date = row[date_idx]
            if isinstance(row_date, str):
                row_date = datetime.strptime(row_date, '%Y-%m-%d')
            row_class_name = row[class_name_idx]

            if (row_type == 'Invoice' and
                start_date <= row_date <= end_date and
                row_class_name != 'Carvart'):
                
                key = (row_project_sopo, row_class_name)
                if key not in result_dict:
                    result_dict[key] = 1

        # Convert the result dictionary to a list of lists
        result = [[key[0], key[1]] for key in result_dict.keys()]

        # Sort the result by ProjectSOPO
        result.sort(key=lambda x: x[0])

        # Add column headers
        result.insert(0, ['ProjectSOPO', 'Class: Name'])

        return result

    except Exception as e:
        return f"Error: {str(e)}"


@xw.func
def SO_with_commissions(file_path):
    """
    Iterates through all worksheets in the specified Excel file,
    extracts values from column D (rows 1-10) and combines them into a single list.
    """
    try:
        # Create Path object and verify file exists
        path = Path(file_path)
        if not path.exists():
            return [["File not found"]]
            
        combined_values = []
        
        # Create new App instance and open workbook
        app = xw.App(visible=False)
        wb = app.books.open(str(path))
        
        try:
            # Iterate through sheets
            combined_values = [
                v for i in range(1, len(wb.sheets))
                for v in wb.sheets[i].range('C1:C10').value
                if v is not None and v != 'SO#'
            ]   
        finally:
            # Clean up
            wb.close()
            app.quit()
            
        # Return results as vertical array
        if combined_values:
            return [[v] for v in combined_values]
        return [["No values found"]]
        
    except Exception as e:
        return [[f"Error: {str(e)}"]]

@xw.func
def Projects_with_commissions(file_path):
    """
    Iterates through all worksheets in the specified Excel file,
    extracts values from column D (rows 1-10) and combines them into a single list.
    """
    try:
        # Create Path object and verify file exists
        path = Path(file_path)
        if not path.exists():
            return [["File not found"]]
            
        combined_values = []
        
        # Create new App instance and open workbook
        app = xw.App(visible=False)
        wb = app.books.open(str(path))
        
        try:
            # Iterate through sheets
            combined_values = [
                v for i in range(1, len(wb.sheets))
                for v in wb.sheets[i].range('D1:D10').value
                if v is not None and v != 'Project #'
            ]   
        finally:
            # Clean up
            wb.close()
            app.quit()
            
        # Return results as vertical array
        if combined_values:
            return [[v] for v in combined_values]
        return [["No values found"]]
        
    except Exception as e:
        return [[f"Error: {str(e)}"]]


@xw.func
@xw.arg('excel_range', ndim=2)
@xw.arg('patterns', ndim=1)
def REGEXSTRPERL(excel_range, patterns):
    def process_cell(cell, patterns):
        cell_str = str(cell)
        lines = cell_str.splitlines()
        maxlen = len(lines)
        if maxlen == 0:
            return ""
        cell_result = lines[maxlen - 1].strip() if maxlen > 0 else ""
        for pattern in patterns:
            match = pcre2.search(pattern, cell_str, flags=pcre2.UNICODE | pcre2.DOTALL)
            if match:
                return match.group(1).strip() if match.groups() else match.group().strip()
        return ""
        
    return [[process_cell(cell, patterns) for cell in row] for row in excel_range]


@xw.func
@xw.arg('excel_range', ndim=2)
@xw.arg('patterns', ndim=1)
@xw.arg('replacement', ndim=0)
def REGEXREPLMPERL(excel_range, patterns, replacement):
    def process_cell(cell, patterns, replacement):
        cell_str = str(cell)
        for pattern in patterns:
            cell_str = pcre2.sub(pattern, replacement, cell_str)
        return cell_str.strip()

    return [[process_cell(cell, patterns, replacement) for cell in row] for row in excel_range]

@xw.func
@xw.arg('excel_range', ndim=2)
@xw.arg('patterns', ndim=1)
@xw.arg('group_index', default=0)
@xw.arg('is_date_pattern', default=True)
def REGEXGRPPERL(excel_range, patterns, group_index=0, is_date_pattern=True):
    def process_cell(cell, patterns, group_index):
        cell_str = str(cell)
        if not cell_str:
            return ""
        
        # Process each pattern
        for pattern in patterns:
            try:
                match = pcre2.search(pattern, cell_str, flags=pcre2.UNICODE | pcre2.DOTALL)
                if match:
                    #print(f"cell_str: {cell_str}")
                    #print(f"pattern: {pattern}")
                    #print(f"match: {match}")
                    # Get the matched text
                    matched_text = match.group(group_index) if group_index != 0 else match.group()
                    if is_date_pattern:
                        # Clean up the matched text
                        if ('/' in matched_text and '-' in matched_text):
                            # If both separators exist, keep only up to the last separator
                            last_slash = matched_text.rfind('/')
                            last_hyphen = matched_text.rfind('-')
                            last_separator = max(last_slash, last_hyphen)
                            if last_separator > 0:
                                # Keep only the part before the last separator
                                matched_text = matched_text[:last_separator]
                    
                    return matched_text.strip()
            except Exception as e:
                return f"Error: {str(e)}"
        
        # No match found
        return ""
        
    return [[process_cell(cell, patterns, group_index) for cell in row] for row in excel_range]


@xw.func
@xw.arg('excel_range', ndim=2)
@xw.arg('patterns', ndim=1)
@xw.arg('group_index', default=0)
@xw.arg('is_date_pattern', default=True)
@xw.arg('replacement', default=None)
def REGEXGRPREPLPERL(excel_range, patterns, group_index=0, is_date_pattern=True, replacement=None):
    def process_cell(cell, patterns, group_index, is_date_pattern, replacement):
        cell_str = str(cell) if cell is not None else ""
        if not cell_str:
            return ""
        
        # Process each pattern
        for pattern in patterns:
            try:
                match = pcre2.search(pattern, cell_str, flags=pcre2.UNICODE | pcre2.DOTALL)
                if match:
                    # Get the matched text
                    matched_text = match.group(group_index) if group_index != 0 else match.group()
                    
                    if is_date_pattern:
                        # Clean up the matched text
                        if '/' in matched_text and '-' in matched_text:
                            # If both separators exist, keep only up to the last separator
                            last_slash = matched_text.rfind('/')
                            last_hyphen = matched_text.rfind('-')
                            last_separator = max(last_slash, last_hyphen)
                            if last_separator > 0:
                                # Keep only the part before the last separator
                                matched_text = matched_text[:last_separator]
                    
                    # If a replacement string is provided, replace the matched text
                    if replacement is not None:
                        return cell_str.replace(matched_text, replacement).strip()
                    
                    return matched_text.strip()
            except Exception as e:
                return f"Error: {str(e)}"
        
        # No match found, return the original cell content
        return cell_str
        
    return [[process_cell(cell, patterns, group_index, is_date_pattern, replacement) for cell in row] for row in excel_range]

# Grok AI (3.0) Chat Session

@xw.func
def perform_substitution_array(range_input, pattern, replacement, group_to_replace=None):
    """
    Performs pcre2 regex substitution on a single-column range and returns an array.
    Args:
        range_input: Single-column Excel range (e.g., G7:G10) as a 1D list
        pattern: Regular expression pattern (string from M5)
        replacement: String to replace the matched pattern or group (e.g., "00")
        group_to_replace: Optional integer specifying which group to replace (default None)
    Returns:
        A 1D list with substituted values for single-column output
    """
    try:
        regex = pcre2.compile(pattern.encode('utf-8'))
        # Treat range_input as a 1D list directly
        input_texts = [str(text) if text is not None else "" for text in range_input]

        result_array = []
        for text in input_texts:
            if group_to_replace is not None and group_to_replace != 0:
                group_to_replace = int(group_to_replace)
                def replacement_builder(match):
                    groups = [match[i].decode('utf-8') if match[i] is not None else ''
                              for i in range(match.group_count() + 1)]
                    if group_to_replace < len(groups):
                        groups[group_to_replace] = replacement
                    return ''.join(groups[0:1] + [f"{groups[i]}" for i in range(1, len(groups))])
                result = regex.sub(lambda m: replacement_builder(m).encode('utf-8'),
                                 text.encode('utf-8')).decode('utf-8')
            else:
                result = regex.sub(replacement.encode('utf-8'),
                                 text.encode('utf-8')).decode('utf-8')
            result_array.append(result)  # Append as single value, not list
        # Convert horizontal 1D list to vertical 2D list using list comprehension
        return [[result] for result in result_array]
    except Exception as e:
        return [f"Error: {str(e)}"]  # Return 1D list with error

# Test locally
if __name__ == "__main__":
    test_input = ["PO 27007 - FRB 30325 - Shipped Glasmrte PO 26561", "", "Dec electric - 12/03/24- 01/03-25", ""]
    test_pattern = r"(?<!\d)(?:0?[1-9]|1[0-2])(?:\/|\-)(?:0?[1-9]|[12]\d|3[01])(?:(?:\/|\-)\d{2,4})?(?!\d)"
    test_replacement = "00"
    test_group = 0
    result = perform_substitution_array(test_input, test_pattern, test_replacement, test_group)
    print("Test result:")
    print(result)

@xw.func
def process_pandas_full_data(file_path):
    # Define column dtypes for efficient reading
    dtype_dict = {
        'Project': str,
        'Class: Name': str,
        'Type': str,
        'Document Number': str,
        'Name': str,
        'Memo': str,
        'Account': str,
        'Clr': str,
        'Split': str,
        'Qty': str,
        'Amount': str
    }

    # Read the CSV
    df = pd.read_csv(file_path, header=0, quotechar='"', dtype=dtype_dict)
    
    # Clean column names
    df.columns = df.columns.str.strip()

    #Sanitise Memo
    if 'Memo' in df.columns:
        df['Memo'] = df['Memo'].str.replace(r'\\', ' ', regex=True)
    
    # Convert Date column to datetime and then to Excel date number
    if 'Date' in df.columns:
        df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y', errors='coerce')
        # Convert to Excel date number (number of days since 1900-01-01)
        df['Date'] = (df['Date'] - pd.Timestamp('1899-12-30')).dt.days
    
    # Clean numeric columns (Qty and Amount)
    numeric_cols = ['Qty', 'Amount']
    for col in numeric_cols:
        if col in df.columns:
            df[col] = (df[col].astype(str)
                      .replace({
                          r',': '',
                          r'\$': '',
                          r'\(': '-',
                          r'\)': '',
                          r'[^\d.-]': ''
                      }, regex=True)
                      .pipe(pd.to_numeric, errors='coerce')
                      .astype(float))
    
    # Convert to output format
    output_data = [df.columns.tolist()] + df.values.tolist()
    
    return output_data

@xw.func
def process_pyarrow_chunks(file_path):
    
    try:
        # Define read options for PyArrow
        read_options = csv.ReadOptions(
            block_size=10 * 1024 * 1024,
            use_threads=True
        )
        
        parse_options = csv.ParseOptions(
            delimiter=',',
            quote_char='"'
        )
        
        convert_options = csv.ConvertOptions(
            strings_can_be_null=True,
            include_columns=None
        )
        
        batches = []
        batch_count = 0
        
        with csv.open_csv(
            file_path,
            read_options=read_options,
            parse_options=parse_options,
            convert_options=convert_options
        ) as reader:
            for batch in reader:
                batch_count += 1
                df = batch.to_pandas()
                
                # Clean column names - remove extra spaces
                df.columns = df.columns.str.strip()
                
                # Process Memo (appears to be all None in sample)
                if 'Memo' in df.columns:
                    df['Memo'] = df['Memo'].fillna('').astype(str)
                    df.loc[df['Memo'] == 'None', 'Memo'] = ''
                    df['Memo'] = (df['Memo']
                                 .str.replace('\\', ' ', regex=False)
                                 .str.replace(r'\s+', ' ', regex=True)
                                 .str.strip())
                
                # Process Date (format: M/D/YYYY)
                if 'Date' in df.columns:
                    try:
                        df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y', errors='coerce')
                        df['Date'] = (df['Date'] - pd.Timestamp('1899-12-30')).dt.days
                    except Exception as e:
                        print(f"Date conversion error: {e}")
                
                # Process Amount (format: $0.00)
                if 'Amount' in df.columns:
                    df['Amount'] = (df['Amount']
                                  .fillna('$0.00')
                                  .astype(str)
                                  .str.replace('$', '', regex=False)
                                  .str.replace(',', '', regex=False)
                                  .str.replace('(', '-', regex=False)
                                  .str.replace(')', '', regex=False)
                                  .str.strip())
                    df['Amount'] = pd.to_numeric(df['Amount'], errors='coerce')
                
                # Process Qty (already numeric but as string)
                if 'Qty' in df.columns:
                    df['Qty'] = pd.to_numeric(df['Qty'], errors='coerce')
                
                # Handle None values in other columns
                for col in df.columns:
                    if col not in ['Date', 'Amount', 'Qty']:
                        df[col] = df[col].fillna('').astype(str)
                        df.loc[df[col] == 'None', col] = ''
                
                # Convert back to PyArrow and append
                processed_batch = pa.Table.from_pandas(df)
                batches.append(processed_batch)
        
        if batches:
            result_table = pa.concat_tables(batches)
            result_df = result_table.to_pandas()
            
            # Debug: Print sample of processed data
            print("\nProcessed data sample:")
            print(result_df.head())
            print("\nColumn dtypes:")
            print(result_df.dtypes)
            
            return [result_df.columns.tolist()] + result_df.values.tolist()
        return [["No data found"]]
        
    except Exception as e:
        print(f"Error in processing: {e}")
        return [["Error: " + str(e)]]

@xw.func
def process_pyarrow_chunks_new(file_path):
    
    # Define read options for PyArrow
    read_options = csv.ReadOptions(
        block_size=10 * 1024 * 1024,  # 10MB chunks
        use_threads=True
    )
    
    parse_options = csv.ParseOptions(
        delimiter=',',
        quote_char='"'
    )
    
    convert_options = csv.ConvertOptions(
        strings_can_be_null=True,
        include_columns=None  # Read all columns
    )
    
    # Read the CSV file in batches
    batches = []
    batch_size = 500000  # Process in large batches
    batch_count = 0
    
    # Create a reader for the CSV file
    with csv.open_csv(
        file_path,
        read_options=read_options,
        parse_options=parse_options,
        convert_options=convert_options
    ) as reader:
        schema = reader.schema
        
        # Clean column names (remove trailing spaces)
        cleaned_names = {name.strip(): name for name in schema.names}
        
        # Get column indices using cleaned names
        memo_idx = schema.get_field_index(cleaned_names.get('Memo')) if 'Memo' in cleaned_names else -1
        date_idx = schema.get_field_index(cleaned_names.get('Date')) if 'Date' in cleaned_names else -1
        qty_idx = schema.get_field_index(cleaned_names.get('Qty')) if 'Qty' in cleaned_names else -1
        amount_idx = schema.get_field_index(cleaned_names.get('Amount')) if 'Amount' in cleaned_names else -1
        
        # Process each batch
        for batch in reader:
            batch_count += 1
            print(f"Processing batch {batch_count}")
            
            # Convert to dictionary for column-wise processing
            batch_dict = batch.to_pydict()
            
            # Process Memo column
            if memo_idx >= 0:
                original_memo_name = cleaned_names.get('Memo')
                memo_array = batch.column(memo_idx)
                if memo_array.null_count < len(memo_array):
                    if not pa.types.is_string(memo_array.type):
                        memo_array = pc.cast(memo_array, pa.string())
                    memo_array = pc.replace_substring_regex(memo_array, r'\\', ' ')
                    batch_dict[original_memo_name] = memo_array.to_pylist()
            
            # Process Date column
            if date_idx >= 0:
                original_date_name = cleaned_names.get('Date')
                date_array = batch.column(date_idx)
                dates = pd.Series(date_array.to_pandas())
                dates = pd.to_datetime(dates, errors='coerce')
                excel_dates = (dates - pd.Timestamp('1899-12-30')).dt.days
                batch_dict[original_date_name] = excel_dates.tolist()
            
            # Process numeric columns
            for col_name, col_idx in [('Qty', qty_idx), ('Amount', amount_idx)]:
                if col_idx >= 0:
                    original_col_name = cleaned_names.get(col_name)
                    col_array = batch.column(col_idx)
                    
                    if not pa.types.is_string(col_array.type):
                        col_array = pc.cast(col_array, pa.string())
                    
                    col_array = pc.replace_substring_regex(col_array, r',', '')
                    col_array = pc.replace_substring_regex(col_array, r'\$', '')
                    col_array = pc.replace_substring_regex(col_array, r'\(', '-')
                    col_array = pc.replace_substring_regex(col_array, r'\)', '')
                    
                    numeric_values = pd.to_numeric(
                        pd.Series(col_array.to_pandas()), 
                        errors='coerce'
                    )
                    
                    batch_dict[original_col_name] = numeric_values.tolist()
            
            # Create a new table from the processed dictionary
            processed_batch = pa.Table.from_pydict(batch_dict)
            batches.append(processed_batch)
            
            # Free memory
            del batch
            del batch_dict
    
    # Combine all batches
    if batches:
        result_table = pa.concat_tables(batches)
        
        # Convert to pandas for output
        result_df = result_table.to_pandas()
        
        # Convert to output format
        output_data = [result_df.columns.tolist()] + result_df.values.tolist()
        
        return output_data
    else:
        return [["No data found"]]


# ==================== FROM new-dll-14052025.py ====================
# Ensure the DLL path is in the system PATH
os.environ['PATH'] = r'D:\Programs\msys64\ucrt64\bin;D:\dev\dll;D:\boost\libs' + ';' + os.environ['PATH']

# Create FFI object
ffi = FFI()

# Define the C declarations - same as before since the function signatures haven't changed
ffi.cdef("""
    void generateRandomNumbersC(int numNumbers, int numThreadGroups, int numThreadsPerGroup);
    unsigned long long* getNumbersC();
    int getNumbersSizeC();
""")

# Load the new DLL
try:
    dll = ffi.dlopen('rdseed_boost_new_claude.dll')
    print("Successfully loaded rdseed_boost_new_claude.dll")
except Exception as e:
    print(f"Error loading DLL: {e}")
    # Fallback to the original DLL if the new one can't be loaded
    try:
        dll = ffi.dlopen('boost_rdseed_ucrt_new.dll')
        print("Falling back to boost_rdseed_ucrt_new.dll")
    except Exception as e2:
        print(f"Error loading fallback DLL: {e2}")
        # Set dll to None so we can check it later
        dll = None

@xw.func
@xw.arg('NUM_NUMBERS', numbers=int)
@xw.arg('NUM_THREAD_GROUPS', numbers=int)
@xw.arg('NUM_THREADS_PER_GROUP', numbers=int)
def intel_rdseed_boost_claude(NUM_NUMBERS, NUM_THREAD_GROUPS, NUM_THREADS_PER_GROUP):
    """
    Generate random numbers using RDSEED instruction with multi-threading via Claude's optimized implementation.
    
    Args:
        NUM_NUMBERS: Total number of random numbers to generate
        NUM_THREAD_GROUPS: Number of thread groups
        NUM_THREADS_PER_GROUP: Number of threads per group
        
    Returns:
        A list of random numbers
    """
    # Verify DLL was loaded
    if dll is None:
        return [["Error: DLL could not be loaded"]]
    
    # Error handling for input parameters
    if NUM_NUMBERS <= 0 or NUM_THREAD_GROUPS <= 0 or NUM_THREADS_PER_GROUP <= 0:
        return [["Error: All input parameters must be positive integers"]]
    
    try:
        # Convert input parameters to integers (needed for proper C++ function calls)
        NUM_NUMBERS = int(NUM_NUMBERS)
        NUM_THREAD_GROUPS = int(NUM_THREAD_GROUPS)
        NUM_THREADS_PER_GROUP = int(NUM_THREADS_PER_GROUP)
        
        # Start timing for performance measurement
        start_time = time.time()
        
        # Call the function to generate random numbers
        dll.generateRandomNumbersC(NUM_NUMBERS, NUM_THREAD_GROUPS, NUM_THREADS_PER_GROUP)
        
        # Get pointer to the generated numbers
        numbers_ptr = dll.getNumbersC()
        numbers_size = dll.getNumbersSizeC()
        
        # Convert C array to Python list
        numbers = [[int(numbers_ptr[i])] for i in range(numbers_size)]
        
        # Display generation time for performance monitoring
        #generation_time = time.time() - start_time
        #numbers.append([f"Generation time: {generation_time:.4f} seconds"])
        
        return numbers
    
    except Exception as e:
        return [[f"Error: {str(e)}"]]

# For testing outside of Excel
if __name__ == "__main__":
    # Simple test with small values
    test_result = intel_rdseed_boost_claude(10000000, 2, 2)
    print(test_result)
#    print(f"Generated {len(test_result)-1} random numbers")
#    if len(test_result) > 3:  # Print a sample of numbers
#        print("Sample numbers:")
#        for i in range(3):
#            print(f"  {test_result[i][0]}")
#    print(test_result[-1][0])  # Print the timing information

# ==================== FROM random-numbers-claude-24052025.py ====================
@xw.func
def RDRAND_EXCEL(min_val, max_val, count):
    """Generate random numbers in range [min_val, max_val] using RDRAND."""
    count = int(count)
    min_val = int(min_val)
    max_val = int(max_val)
    
    s = rdrand.RdRandom()  # Create instance once
    random_numbers = []
    
    for _ in range(count):
        random_number = s.get_bits(64) % (max_val - min_val + 1) + min_val
        random_numbers.append(random_number)
    
    return [[num] for num in random_numbers]


@xw.func
def RDSEED_FLOAT(min_val, max_val, count):
    """Generate random floating point numbers using RDSEED."""
    count = int(count)
    min_val = float(min_val)
    max_val = float(max_val)
    
    s = rdrand.RdSeedom()  # Create instance once
    random_numbers = []
    
    for _ in range(count):
        # Get 52 bits for float precision, convert to [0,1) range
        random_float = s.get_bits(52) / (2**52)
        # Scale to desired range
        scaled_float = min_val + (max_val - min_val) * random_float
        random_numbers.append(scaled_float)
    
    return [[num] for num in random_numbers]

@xw.func
def RDRAND_FLOAT(min_val, max_val, count):
    """Generate random floating point numbers using RDRAND."""
    count = int(count)
    min_val = float(min_val)
    max_val = float(max_val)
    
    s = rdrand.RdRandom()  # Create instance once
    random_numbers = []
    
    for _ in range(count):
        # Get 52 bits for float precision, convert to [0,1) range
        random_float = s.get_bits(52) / (2**52)
        # Scale to desired range
        scaled_float = min_val + (max_val - min_val) * random_float
        random_numbers.append(scaled_float)
    
    return [[num] for num in random_numbers]

@xw.func
def RDSEED_DICE(num_dice, num_sides):
    """Simulate dice rolls using RDSEED."""
    num_dice = int(num_dice)
    num_sides = int(num_sides)
    
    s = rdrand.RdSeedom()  # Create instance once
    results = []
    total = 0
    
    for i in range(num_dice):
        roll = s.get_bits(32) % num_sides + 1
        results.append([f"Die {i+1}", roll])
        total += roll
    
    results.append(["Total", total])
    return results

@xw.func
def RDSEED_COIN(count):
    """Simulate coin flips using RDSEED."""
    count = int(count)
    
    s = rdrand.RdSeedom()  # Create instance once
    results = []
    heads_count = 0
    
    for i in range(count):
        flip = s.get_bits(1)  # Just 1 bit needed
        result = "Heads" if flip == 1 else "Tails"
        results.append([f"Flip {i+1}", result])
        if flip == 1:
            heads_count += 1
    
    results.append(["Summary", f"{heads_count} Heads, {count - heads_count} Tails"])
    return results

@xw.func
def RDSEED_BYTES(num_bytes):
    """Generate random bytes using RDSEED."""
    num_bytes = int(num_bytes)
    
    s = rdrand.RdSeedom()  # Create instance once
    random_bytes = s.get_bytes(num_bytes)
    
    # Convert bytes to list of integers for Excel display
    result = []
    for i, byte_val in enumerate(random_bytes):
        result.append([f"Byte {i+1}", byte_val])
    
    return result

@xw.func
def RDRAND_BYTES(num_bytes):
    """Generate random bytes using RDRAND."""
    num_bytes = int(num_bytes)
    
    s = rdrand.RdRandom()  # Create instance once
    random_bytes = s.get_bytes(num_bytes)
    
    # Convert bytes to list of integers for Excel display
    result = []
    for i, byte_val in enumerate(random_bytes):
        result.append([f"Byte {i+1}", byte_val])
    
    return result

@xw.func
def RDSEED_M(rows, cols, min_val, max_val):
    """Generate a matrix of random numbers using RDSEED."""
    rows = int(rows)
    cols = int(cols)
    min_val = int(min_val)
    max_val = int(max_val)
    
    s = rdrand.RdSeedom()  # Create instance once
    result = []
    
    for i in range(rows):
        row = []
        for j in range(cols):
            random_number = s.get_bits(64) % (max_val - min_val + 1) + min_val
            row.append(random_number)
        result.append(row)
    
    return result

@xw.func
def RDRAND_M(rows, cols, min_val, max_val):
    """Generate a matrix of random numbers using RDRAND."""
    rows = int(rows)
    cols = int(cols)
    min_val = int(min_val)
    max_val = int(max_val)
    
    s = rdrand.RdRandom()  # Create instance once
    result = []
    
    for i in range(rows):
        row = []
        for j in range(cols):
            random_number = s.get_bits(64) % (max_val - min_val + 1) + min_val
            row.append(random_number)
        result.append(row)
    
    return result

# ==================== FROM company_analytics.py ====================
@xw.func
@xw.arg('data', pd.DataFrame, index=False, header=True)
def company_summary(data):
    """
    Generate company-wise summary with CAGR, Standard Deviation, and latest values
    
    Parameters:
    data: DataFrame with columns - Capitaline Code, Company Name, Date, High Price, Market Cap, Company Long Name, Debt-Equity Ratio
    
    Returns:
    DataFrame with summary statistics
    """
    
    print(f"Initial data shape: {data.shape}")
    print(f"Initial columns: {list(data.columns)}")
    
    # Check for 360 ONE before any processing
    print("\n=== DEBUGGING 360 ONE ===")
    if 'Company Name' in data.columns:
        companies_with_360 = data[data['Company Name'].astype(str).str.contains('360', na=False)]
        print(f"Rows containing '360' in Company Name: {len(companies_with_360)}")
        if len(companies_with_360) > 0:
            print("Sample rows with '360':")
            print(companies_with_360[['Capitaline Code', 'Company Name']].head())
    
    if 'Capitaline Code' in data.columns:
        code_66008_rows = data[data['Capitaline Code'].astype(str).str.contains('66008', na=False)]
        print(f"Rows with code 66008: {len(code_66008_rows)}")
        if len(code_66008_rows) > 0:
            print("Sample rows with code 66008:")
            print(code_66008_rows[['Capitaline Code', 'Company Name']].head())
    
    # Try different approaches to find the missing data
    print(f"Unique Capitaline Codes (first 20): {sorted(data['Capitaline Code'].unique())[:20]}")
    print(f"Data types: {data.dtypes}")
    
    # Check for any NaN values that might cause issues
    print(f"NaN values per column:")
    for col in data.columns:
        nan_count = data[col].isna().sum()
        if nan_count > 0:
            print(f"  {col}: {nan_count} NaN values")
    
    # Clean data more aggressively
    print("\n=== CLEANING DATA ===")
    
    # Remove rows with essential NaN values
    essential_cols = ['Capitaline Code', 'Company Name', 'Date', 'High Price']
    initial_rows = len(data)
    data = data.dropna(subset=essential_cols)
    print(f"Removed {initial_rows - len(data)} rows with NaN in essential columns")
    
    # Convert data types more carefully
    try:
        data['Capitaline Code'] = pd.to_numeric(data['Capitaline Code'], errors='coerce')
        data = data.dropna(subset=['Capitaline Code'])  # Remove rows where conversion failed
        data['Capitaline Code'] = data['Capitaline Code'].astype(int)
    except Exception as e:
        print(f"Error converting Capitaline Code: {e}")
    
    # Clean company names
    data['Company Name'] = data['Company Name'].astype(str).str.strip()
    
    # Ensure Date column is datetime
    data['Date'] = pd.to_datetime(data['Date'], errors='coerce')
    data = data.dropna(subset=['Date'])
    
    print(f"Final data shape after cleaning: {data.shape}")
    
    # Check again for 360 ONE after cleaning
    print("\n=== AFTER CLEANING ===")
    companies_with_360_clean = data[data['Company Name'].str.contains('360', na=False)]
    print(f"Rows containing '360' after cleaning: {len(companies_with_360_clean)}")
    if len(companies_with_360_clean) > 0:
        print("360 companies after cleaning:")
        print(companies_with_360_clean[['Capitaline Code', 'Company Name']].drop_duplicates())
    
    code_66008_clean = data[data['Capitaline Code'] == 66008]
    print(f"Rows with code 66008 after cleaning: {len(code_66008_clean)}")
    
    # Group by company
    grouped = data.groupby(['Capitaline Code', 'Company Name'])
    
    print(f"\nNumber of groups: {len(grouped)}")
    
    results = []
    
    for (code, company), group in grouped:
        if code == 66008 or '360' in str(company):
            print(f"*** FOUND TARGET: Code={code}, Company='{company}', Rows={len(group)} ***")
        
        # Sort by date to ensure proper ordering
        group = group.sort_values('Date')
        
        # Get earliest and latest records
        earliest_record = group.iloc[0]
        latest_record = group.iloc[-1]
        
        # Calculate CAGR using your specific formula
        latest_price = latest_record['High Price']
        earliest_price = earliest_record['High Price']
        latest_date = latest_record['Date']
        earliest_date = earliest_record['Date']
        
        # Days difference + 1
        days_diff = (latest_date - earliest_date).days + 1
        years_times_quarters = (days_diff / 365) * 4
        
        if years_times_quarters > 0 and earliest_price > 0:
            cagr = ((latest_price / earliest_price) ** (1 / years_times_quarters) - 1) * 4 * 100
        else:
            cagr = np.nan
        
        # Calculate Standard Deviation and Mean of prices
        mean_price = group['High Price'].mean()
        std_dev = group['High Price'].std()
        
        # Calculate Coefficient of Variation (COV)
        cov = std_dev / mean_price if mean_price > 0 else np.nan
        
        # Get latest values
        latest_market_cap = latest_record['Market Cap']
        latest_debt_equity = latest_record['Debt-Equity Ratio']
        
        results.append({
            'Capitaline Code': code,
            'Company Name': company,
            'Price CAGR': round(cagr, 4) if not np.isnan(cagr) else None,
            'Mean': round(mean_price, 2),
            'Standard Deviation': round(std_dev, 2),
            'COV': round(cov, 4) if not np.isnan(cov) else None,
            'Latest Price': round(latest_price, 2),
            'Latest Debt Equity Ratio': round(latest_debt_equity, 2),
            'Latest Market Cap': round(latest_market_cap, 2)
        })
    
    result_df = pd.DataFrame(results)
    print(f"\nFinal results shape: {result_df.shape}")
    
    # Check if 360 ONE is in final results
    final_360 = result_df[result_df['Company Name'].str.contains('360', na=False)]
    print(f"360 companies in final results: {len(final_360)}")
    
    return result_df

@xw.func
@xw.arg('range_data', xw.Range)
def company_summary_from_range(range_data):
    """
    Alternative function that takes a range as input
    Usage: =company_summary_from_range(A1:G100)
    """
    
    # Convert range to DataFrame
    data = range_data.options(pd.DataFrame, header=1, index=False).value
    
    # Rename columns to match expected format
    expected_columns = ['Capitaline Code', 'Company Name', 'Date', 'High Price', 'Market Cap', 'Company Long Name', 'Debt-Equity Ratio']
    data.columns = expected_columns[:len(data.columns)]
    
    return company_summary(data)

# Additional helper function for Excel usage
@xw.func
def calculate_price_cagr(latest_price, earliest_price, latest_date, earliest_date):
    """
    Calculate CAGR using your specific formula
    Usage: =calculate_price_cagr(latest_price, earliest_price, latest_date, earliest_date)
    """
    try:
        # Convert dates to datetime if they're not already
        if isinstance(latest_date, str):
            latest_date = pd.to_datetime(latest_date)
        if isinstance(earliest_date, str):
            earliest_date = pd.to_datetime(earliest_date)
        
        days_diff = (latest_date - earliest_date).days + 1
        years_times_quarters = (days_diff / 365) * 4
        
        if years_times_quarters > 0 and earliest_price > 0:
            cagr = ((latest_price / earliest_price) ** (1 / years_times_quarters) - 1) * 4 * 100
            return round(cagr, 4)
        else:
            return None
    except:
        return None

# Simple debugging function to check data
@xw.func
@xw.arg('range_data', xw.Range)
def debug_companies(range_data):
    """
    Debug function to check what companies are in the data
    Usage: =debug_companies(A1:G100)
    """
    try:
        # Convert range to DataFrame
        data = range_data.options(pd.DataFrame, header=1, index=False).value
        
        # Check if we have the expected columns
        print("Columns in data:", list(data.columns))
        
        # Rename columns to match expected format
        expected_columns = ['Capitaline Code', 'Company Name', 'Date', 'High Price', 'Market Cap', 'Company Long Name', 'Debt-Equity Ratio']
        data.columns = expected_columns[:len(data.columns)]
        
        # Clean and check unique companies
        data['Company Name'] = data['Company Name'].astype(str).str.strip()
        data['Capitaline Code'] = data['Capitaline Code'].astype(int)
        
        unique_companies = data[['Capitaline Code', 'Company Name']].drop_duplicates().sort_values('Capitaline Code')
        
        print("Unique companies found:")
        for _, row in unique_companies.iterrows():
            print(f"Code: {row['Capitaline Code']}, Name: '{row['Company Name']}'")
        
        # Check specifically for 360 ONE
        filtered_360 = data[data['Company Name'].str.contains('360', na=False)]
        print(f"\nRows containing '360': {len(filtered_360)}")
        if len(filtered_360) > 0:
            print("Sample 360 rows:")
            print(filtered_360[['Capitaline Code', 'Company Name']].head())
        
        return f"Found {len(unique_companies)} unique companies. Check console for details."
        
    except Exception as e:
        return f"Error: {str(e)}"
def setup_xlwings():
    """
    Setup function to establish xlwings connection
    Run this in Python before using UDFs in Excel
    """
    # This will start the Excel connection
    wb = xw.Book.caller()  # Connect to the calling workbook
    return wb

if __name__ == "__main__":
    # For testing purposes - you can run this to test the function
    # Create sample data similar to your example
    sample_data = pd.DataFrame({
        'Capitaline Code': [66008, 66008, 66008, 107, 107, 107],
        'Company Name': ['360 ONE', '360 ONE', '360 ONE', 'A B Real Estate', 'A B Real Estate', 'A B Real Estate'],
        'Date': ['2023-07-12', '2023-07-13', '2023-07-14', '2022-06-16', '2022-06-17', '2022-06-20'],
        'High Price': [505, 506.85, 514.85, 787, 761, 750.75],
        'Market Cap': [17937.68, 17916.26, 18228.64, 8401.75, 8319.65, 8154.34],
        'Company Long Name': ['360 ONE WAM Ltd', '360 ONE WAM Ltd', '360 ONE WAM Ltd', 'Aditya Birla Real Estate Ltd', 'Aditya Birla Real Estate Ltd', 'Aditya Birla Real Estate Ltd'],
        'Debt-Equity Ratio': [0.43, 0.43, 0.43, 0.27, 0.27, 0.27]
    })
    
    result = company_summary(sample_data)
    print(result)

# ==================== FROM BSE.py ====================
# Global variables for caching
_processed_data: Optional[pd.DataFrame] = None
_last_file_path: Optional[str] = None
_processing_lock = threading.Lock()

class BSEDataProcessor:
    """Optimized BSE data processor that includes single data point companies."""
    
    def __init__(self):
        self.chunk_size = 50000
        self.required_columns = ['Company Name', 'Date', 'Open Price', 'High Price', 'Low Price', 'Close Price']
        self.optional_columns = ['Market Cap', 'Debt-Equity Ratio', 'Capitaline Code']
    
    def _parse_date_efficiently(self, date_series: pd.Series) -> pd.Series:
        """Efficient date parsing with multiple format attempts."""
        formats = ['%d/%m/%Y', '%Y-%m-%d', '%m/%d/%Y', '%d-%m-%Y']
        
        for fmt in formats:
            try:
                return pd.to_datetime(date_series, format=fmt, errors='coerce')
            except:
                continue
        
        return pd.to_datetime(date_series, errors='coerce')
    
    def _calculate_metrics(self, dates: np.ndarray, prices: np.ndarray) -> Dict[str, float]:
        """Calculate financial metrics efficiently with handling for single data points."""
        if len(dates) < 1 or len(prices) < 1:
            return {}
        
        # Remove invalid data
        valid_mask = (prices > 0) & (~np.isnan(prices))
        valid_count = np.sum(valid_mask)
        
        if valid_count < 1:
            return {}
        
        dates = dates[valid_mask]
        prices = prices[valid_mask]
        
        # Sort by date
        sort_idx = np.argsort(dates)
        dates = dates[sort_idx]
        prices = prices[sort_idx]
        
        # Handle single data point case
        if len(prices) == 1:
            return {
                'cagr': 0.0,
                'mean_price': round(prices[0], 4),
                'std_dev': 0.0,
                'cov': 0.0,
                'latest_price': round(prices[0], 4),
                'data_points': 1,
                'single_data_point': True
            }
        
        # Calculate CAGR (Quarterly basis) for multiple data points
        days_diff = (dates[-1] - dates[0]).astype('timedelta64[D]').astype(int) + 1
        years = days_diff / 365
        
        cagr = None
        if years > 0 and prices[0] > 0:
            quarters = years * 4
            if quarters > 0:
                cagr = ((prices[-1] / prices[0]) ** (1 / quarters) - 1) * 4 * 100
        
        # Handle edge case where start and end dates are the same
        if years == 0:
            if len(prices) > 1:
                cagr = ((prices[-1] / prices[0]) - 1) * 100
            else:
                cagr = 0.0
        
        # Calculate other statistics
        mean_price = np.mean(prices)
        std_dev = np.std(prices, ddof=1)
        cov = std_dev / mean_price if mean_price > 0 else 0.0
        
        return {
            'cagr': round(cagr, 6) if cagr is not None else 0.0,
            'mean_price': round(mean_price, 6),
            'std_dev': round(std_dev, 6),
            'cov': round(cov, 6),
            'latest_price': round(prices[-1], 4),
            'data_points': len(prices),
            'single_data_point': False
        }
    
    def process_file(self, file_path: str) -> Dict[str, Any]:
        """Process CSV file with optimized memory usage."""
        file_path = os.path.normpath(file_path)
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")
        
        file_size_gb = os.path.getsize(file_path) / (1024**3)
        
        # Adjust chunk size based on file size
        if file_size_gb > 2:
            chunk_size = 25000
        elif file_size_gb > 1:
            chunk_size = 40000
        else:
            chunk_size = self.chunk_size
        
        company_data = {}
        processed_chunks = 0
        
        try:
            csv_iterator = pd.read_csv(
                file_path,
                delimiter='\t',
                chunksize=chunk_size,
                low_memory=False,
                dtype={
                    'Company Name': 'string',
                    'Open Price': 'float32',
                    'High Price': 'float32',
                    'Low Price': 'float32',
                    'Close Price': 'float32',
                    'Market Cap': 'float32',
                    'Debt-Equity Ratio': 'float32'
                },
                usecols=lambda x: x in self.required_columns + self.optional_columns
            )
            
            for chunk in csv_iterator:
                processed_chunks += 1
                
                # Skip chunks missing required columns
                if not all(col in chunk.columns for col in self.required_columns):
                    continue
                
                # Calculate average price using vectorized operations
                price_cols = ['Open Price', 'High Price', 'Low Price', 'Close Price']
                chunk['Average_Price'] = chunk[price_cols].mean(axis=1)
                
                # Parse dates efficiently
                chunk['Date'] = self._parse_date_efficiently(chunk['Date'])
                
                # Clean data
                chunk = chunk.dropna(subset=['Date', 'Average_Price'])
                chunk = chunk[chunk['Average_Price'] > 0]
                
                # Group by Company and Date to handle multiple entries per day
                if len(chunk) > 0:
                    chunk = chunk.groupby(['Company Name', 'Date']).agg({
                        'Average_Price': 'mean',
                        'Market Cap': 'last',
                        'Debt-Equity Ratio': 'last',
                        'Capitaline Code': 'first'
                    }).reset_index()
                
                if chunk.empty:
                    continue
                
                # Process each company in the chunk
                for company_name, group in chunk.groupby('Company Name'):
                    if pd.isna(company_name) or company_name == '' or str(company_name).strip() == '':
                        continue
                    
                    company_name = str(company_name).strip()
                    
                    if company_name not in company_data:
                        company_data[company_name] = {
                            'dates': [],
                            'prices': [],
                            'market_caps': [],
                            'debt_equity': [],
                            'capitaline_code': None
                        }
                    
                    # Store data
                    company_data[company_name]['dates'].extend(group['Date'].values)
                    company_data[company_name]['prices'].extend(group['Average_Price'].values)
                    
                    # Handle optional columns
                    if 'Market Cap' in group.columns:
                        company_data[company_name]['market_caps'].extend(
                            group['Market Cap'].fillna(0).values
                        )
                    if 'Debt-Equity Ratio' in group.columns:
                        company_data[company_name]['debt_equity'].extend(
                            group['Debt-Equity Ratio'].fillna(0).values
                        )
                    if 'Capitaline Code' in group.columns and company_data[company_name]['capitaline_code'] is None:
                        codes = group['Capitaline Code'].dropna()
                        if not codes.empty:
                            company_data[company_name]['capitaline_code'] = codes.iloc[0]
        
        except Exception as e:
            raise
        
        return self._calculate_company_metrics(company_data)
    
    def _calculate_company_metrics(self, company_data: Dict) -> Dict[str, Any]:
        """Calculate metrics for all companies including single data point companies."""
        results = []
        single_data_point_companies = 0
        
        for company_name, data in company_data.items():
            # Skip only if no data at all
            if len(data['dates']) < 1:
                continue
            
            # Convert to numpy arrays
            dates = np.array(data['dates'])
            prices = np.array(data['prices'])
            
            # Calculate metrics (now handles single data points)
            metrics = self._calculate_metrics(dates, prices)
            if not metrics:
                continue
            
            # Track single data point companies
            if metrics.get('data_points', 0) == 1:
                single_data_point_companies += 1
            
            # Get latest values
            latest_market_cap = data['market_caps'][-1] if data['market_caps'] else 0
            latest_debt_equity = data['debt_equity'][-1] if data['debt_equity'] else 0
            
            # Create result
            result_row = {
                'Capitaline_Code': data['capitaline_code'] or '',
                'Company_Name': company_name,
                'Price_CAGR': metrics['cagr'],
                'Mean_Price': metrics['mean_price'],
                'Standard_Deviation': metrics['std_dev'],
                'COV': metrics['cov'],
                'Latest_Price': metrics['latest_price'],
                'Latest_Debt_Equity': round(latest_debt_equity, 2),
                'Latest_Market_Cap': round(latest_market_cap, 2),
                'Data_Points': metrics['data_points'],
                'Single_Data_Point': metrics.get('single_data_point', False)
            }
            
            results.append(result_row)
        
        return {
            'dataframe': pd.DataFrame(results),
            'total_companies': len(results),
            'processed_companies': len(company_data),
            'single_data_point_companies': single_data_point_companies
        }

# Initialize processor
processor = BSEDataProcessor()

@xw.func
def process_csv_file(file_path: str) -> str:
    """Process the large CSV file and cache the results"""
    global _processed_data, _last_file_path
    
    if not file_path:
        return "Error: Please provide file_path argument"
    
    file_path = os.path.normpath(file_path)
    
    with _processing_lock:
        try:
            # Check if already processed
            if _processed_data is not None and _last_file_path == file_path:
                single_point_count = (_processed_data['Data_Points'] == 1).sum()
                return f"Data already loaded ({len(_processed_data)} companies, {single_point_count} single data point)"
            
            # Process the file
            result = processor.process_file(file_path)
            _processed_data = result['dataframe']
            _last_file_path = file_path
            
            single_point_count = result.get('single_data_point_companies', 0)
            return f"Success: {result['total_companies']} companies ({single_point_count} single data point)"
            
        except Exception as e:
            return f"Error: {str(e)}"

@xw.func
def get_single_data_point_companies(file_path: str) -> List[List]:
    """Get list of companies with single data points"""
    global _processed_data, _last_file_path
    
    if not file_path:
        return [["Error: Please provide file_path argument"]]
    
    file_path = os.path.normpath(file_path)
    
    if _processed_data is None or _last_file_path != file_path:
        process_result = process_csv_file(file_path)
        if process_result.startswith("Error"):
            return [[process_result]]
    
    try:
        single_point_companies = _processed_data[_processed_data['Data_Points'] == 1].copy()
        
        if single_point_companies.empty:
            return [["No companies with single data point found"]]
        
        result = [["Company Name", "Latest Price", "Market Cap", "Debt-Equity Ratio"]]
        
        for _, row in single_point_companies.iterrows():
            result.append([
                row['Company_Name'],
                row['Latest_Price'],
                row['Latest_Market_Cap'],
                row['Latest_Debt_Equity']
            ])
        
        return result
    
    except Exception as e:
        return [[f"Error: {str(e)}"]]

@xw.func
def get_company_list(file_path: str) -> List[str]:
    """Get list of all company names"""
    global _processed_data, _last_file_path
    
    if not file_path:
        return ["Error: Please provide file_path argument"]
    
    file_path = os.path.normpath(file_path)
    
    if _processed_data is None or _last_file_path != file_path:
        process_result = process_csv_file(file_path)
        if process_result.startswith("Error"):
            return [process_result]
    
    return sorted(_processed_data['Company_Name'].tolist())

@xw.func
def get_company_data(file_path: str, company_name: str, metric: str = "all"):
    """Get specific data for a company"""
    global _processed_data, _last_file_path
    
    if not file_path:
        return "Error: Please provide file_path argument"
    
    file_path = os.path.normpath(file_path)
    
    if _processed_data is None or _last_file_path != file_path:
        process_result = process_csv_file(file_path)
        if process_result.startswith("Error"):
            return process_result
    
    company_data = _processed_data[_processed_data['Company_Name'].str.contains(company_name, case=False, na=False)]
    
    if company_data.empty:
        return f"Error: Company '{company_name}' not found"
    
    row = company_data.iloc[0]
    
    metric_map = {
        "CAGR": "Price_CAGR",
        "COV": "COV", 
        "MEAN_PRICE": "Mean_Price",
        "LATEST_PRICE": "Latest_Price",
        "LATEST_DEBT_EQUITY": "Latest_Debt_Equity",
        "LATEST_MARKET_CAP": "Latest_Market_Cap",
        "DATA_POINTS": "Data_Points"
    }
    
    if metric.lower() == "all":
        result = [
            row['Price_CAGR'],
            row['COV'],
            row['Mean_Price'],
            row['Latest_Price'],
            row['Latest_Debt_Equity'],
            row['Latest_Market_Cap'],
            row['Data_Points']
        ]
        
        if row['Data_Points'] == 1:
            result.append("Single data point")
        
        return result
    elif metric.upper() in metric_map:
        value = row[metric_map[metric.upper()]]
        if row['Data_Points'] == 1 and metric.upper() in ['CAGR', 'COV']:
            return f"{value} (Single data point)"
        return value
    else:
        return f"Error: Unknown metric '{metric}'"

@xw.func
def get_top_companies(file_path: str, metric: str = "CAGR", count: int = 10, include_single_point: bool = True) -> List[List]:
    """Get top companies by specified metric"""
    global _processed_data, _last_file_path
    
    if not file_path:
        return [["Error: Please provide file_path argument"]]
    
    file_path = os.path.normpath(file_path)
    
    if _processed_data is None or _last_file_path != file_path:
        process_result = process_csv_file(file_path)
        if process_result.startswith("Error"):
            return [[process_result]]
    
    metric_map = {
        "CAGR": "Price_CAGR",
        "COV": "COV",
        "MEAN_PRICE": "Mean_Price", 
        "LATEST_PRICE": "Latest_Price",
        "LATEST_MARKET_CAP": "Latest_Market_Cap"
    }
    
    if metric.upper() not in metric_map:
        return [[f"Error: Unknown metric '{metric}'"]]
    
    column_name = metric_map[metric.upper()]
    
    try:
        # Filter data based on single point preference
        if include_single_point:
            filtered_data = _processed_data.dropna(subset=[column_name])
        else:
            filtered_data = _processed_data[
                (_processed_data['Data_Points'] > 1) & 
                (_processed_data[column_name].notna())
            ]
        
        top_companies = filtered_data.nlargest(count, column_name)
        
        result = [["Company Name", metric.upper(), "Data Points"]]
        
        for _, row in top_companies.iterrows():
            result.append([
                row['Company_Name'], 
                row[column_name], 
                row['Data_Points']
            ])
        
        return result
    
    except Exception as e:
        return [[f"Error: {str(e)}"]]

@xw.func
def get_all_data_as_array(file_path: str) -> List[List]:
    """Get all processed data as a dynamic array for Excel"""
    global _processed_data, _last_file_path
    
    if not file_path:
        return [["Error: Please provide file_path argument"]]
    
    file_path = os.path.normpath(file_path)
    
    if _processed_data is None or _last_file_path != file_path:
        process_result = process_csv_file(file_path)
        if process_result.startswith("Error"):
            return [[process_result]]
    
    try:
        result = []
        
        # Add headers (exclude the Single_Data_Point column for cleaner output)
        headers = [col for col in _processed_data.columns if col != 'Single_Data_Point']
        result.append(headers)
        
        # Add data rows (limit to prevent Excel crashes)
        max_rows = min(len(_processed_data), 100000)
        for i in range(max_rows):
            row = _processed_data.iloc[i]
            data_row = []
            for col in headers:
                value = row[col]
                if pd.isna(value) or value is None:
                    data_row.append("")
                else:
                    data_row.append(value)
            result.append(data_row)
        
        if len(_processed_data) > max_rows:
            result.append([f"... and {len(_processed_data) - max_rows} more rows"])
        
        return result
    
    except Exception as e:
        return [[f"Error: {str(e)}"]]

@xw.func
def clear_cache() -> str:
    """Clear cached data"""
    global _processed_data, _last_file_path
    
    with _processing_lock:
        _processed_data = None
        _last_file_path = None
    
    return "Cache cleared"

@xw.func
def check_processing_status() -> str:
    """Check if processing is complete"""
    global _processed_data
    
    if _processed_data is None:
        return "No data loaded"
    else:
        single_count = (_processed_data['Data_Points'] == 1).sum()
        return f"Ready: {len(_processed_data)} companies ({single_count} single data point)"

if __name__ == "__main__":
    xw.serve()

# ==================== FROM investingapicall.py ====================
@xw.func
def get_url_data_id_new_2(investingid, from_date, to_date):
    # Convert date strings to MM/DD/YYYY format
    from_date = datetime.strptime(from_date, "%d/%m/%Y").strftime("%m/%d/%Y")
    to_date = datetime.strptime(to_date, "%d/%m/%Y").strftime("%m/%d/%Y")

    # Construct the updated URL
    url = (
        f"http://api.scraperlink.com/investpy/?email=asharindani51@gmail.com"
        f"&type=historical_data&product=stocks&country=india&id={investingid}"
        f"&from_date={from_date}&to_date={to_date}"
    )

    # Fetch data from the API
    response = requests.get(url)
    json_output = response.json()
    data_list = json_output['data']

    # Convert data_list into a pandas dataframe
    df = pd.DataFrame(data_list)

    # Remove commas from numeric columns
    int_float_columns = [
        'last_close','last_open','last_max','last_min',
        'change_precent','last_closeRaw', 'last_openRaw',
        'last_maxRaw', 'last_minRaw', 'change_precentRaw'
    ]
    for col in int_float_columns:
        df[col] = df[col].astype(str).str.replace(',', '').astype('float')

    # Format columns
    df['direction_color'] = df['direction_color'].astype(str)
    df['rowDate'] = pd.to_datetime(df['rowDate'], format='%b %d, %Y').dt.date
    df['rowDateTimestamp'] = pd.to_datetime(df['rowDateTimestamp']).dt.date

    # Create a matrix (list of lists) from the dataframe
    matrix = df.values.tolist()
    headers = df.columns.tolist()
    matrix.insert(0, headers)

    return matrix

#================== From hybrid_sql_approach.py =============================
@xw.func
def sql_enhanced(range_ref, sql_statement, use_chunking=False, chunk_size=50000):
    """
    Enhanced SQL function that handles both range addresses and Table references
    Usage: =sql_enhanced("A1:C978721", "SELECT * FROM a WHERE [Firm / Headquarters] LIKE '%Texas%'")
    Usage: =sql_enhanced(Table1[#All], "SELECT * FROM a WHERE [Firm / Headquarters] LIKE '%Texas%'")
    Usage with chunking: =sql_enhanced("A1:C978721", "SELECT * FROM a WHERE [Firm / Headquarters] LIKE '%Texas%'", TRUE, 50000)
    
    Args:
        range_ref: Range address string OR Excel range/table reference
        sql_statement: SQL query string (same syntax as native =sql())
        use_chunking: Boolean, whether to force chunked processing
        chunk_size: Size of chunks for large datasets
    
    Returns: Query results (attempts native xlwings sql first, custom fallback)
    """
    try:
        wb = xw.Book.caller()
        sheet = wb.sheets.active
        
        # Handle different types of range references
        if isinstance(range_ref, str):
            # String address like "A1:C978721"
            range_obj = sheet.range(range_ref)
        else:
            # Already a range object or table reference - get its address first
            try:
                # If it's already a range object, use it directly
                if hasattr(range_ref, 'address'):
                    range_obj = sheet.range(range_ref.address)
                else:
                    # Try to convert the reference to a range
                    range_obj = sheet.range(range_ref)
            except:
                # Fallback: treat as direct data
                if hasattr(range_ref, '__iter__') and not isinstance(range_ref, str):
                    # It's already data, process directly
                    return sql_query_optimized_internal(range_ref, sql_statement)
                else:
                    raise ValueError(f"Unable to process range reference: {range_ref}")
        
        # Get the data from the range
        data_range = range_obj.value
        
        # Skip native xlwings sql for now since it doesn't handle table refs well
        # Go straight to our custom implementation
        if use_chunking or len(data_range) > 100000:  # Use chunking for large datasets
            return sql_query_chunked_internal(data_range, sql_statement, chunk_size=chunk_size)
        else:
            return sql_query_optimized_internal(data_range, sql_statement)
                
    except Exception as outer_error:
        return [["Error:", str(outer_error)]]

@xw.func 
def sql_table_fixed(table_reference, sql_statement, chunk_size=50000):
    """
    Fixed SQL function for Excel Tables - handles various table reference formats
    Usage: =sql_table_fixed("Table14", F2)
    Usage: =sql_table_fixed("Table14[#All]", F2)  -- also works
    
    Args:
        table_reference: Table name (e.g., "Table14") or reference ("Table14[#All]")
        sql_statement: SQL query string
        chunk_size: Chunk size for large tables
    
    Returns: Query results
    """
    try:
        wb = xw.Book.caller()
        sheet = wb.sheets.active
        
        # Clean up the table reference - extract just the table name
        if isinstance(table_reference, str):
            # Remove [#All] if present and clean up
            table_name = table_reference.replace("[#All]", "").replace("[#Headers]", "").strip()
        else:
            table_name = str(table_reference)
        
        # Try different approaches to get the table data
        table_range = None
        
        # Method 1: Try direct table reference on current sheet
        try:
            table_range = sheet.range(f"{table_name}[#All]")
        except:
            pass
        
        # Method 2: Search all sheets for the table
        if table_range is None:
            for ws in wb.sheets:
                try:
                    table_range = ws.range(f"{table_name}[#All]")
                    break
                except:
                    continue
        
        # Method 3: If still not found, try without [#All]
        if table_range is None:
            try:
                table_range = sheet.range(table_name)
            except:
                pass
        
        if table_range is None:
            return [["Error:", f"Could not find table '{table_name}'. Check table name and ensure it exists."]]
        
        # Get the data
        data_range = table_range.value
        
        # Validate we have data
        if not data_range or len(data_range) == 0:
            return [["Error:", "No data found in table"]]
        
        # Process the SQL query directly here
        try:
            # Convert to DataFrame
            df = pd.DataFrame(data_range[1:], columns=data_range[0])
            df = df.dropna(how='all')
            
            # Clean column names for SQLite compatibility
            original_columns = df.columns.tolist()
            clean_columns = []
            column_mapping = {}
            
            for col in original_columns:
                clean_col = re.sub(r'[^\w]', '_', str(col))
                clean_columns.append(clean_col)
                column_mapping[col] = clean_col
            
            df.columns = clean_columns
            
            # Update SQL statement to use clean column names
            updated_sql = sql_statement
            for original, clean in column_mapping.items():
                pattern = rf'\[{re.escape(original)}\]'
                updated_sql = re.sub(pattern, clean, updated_sql)
                pattern = rf'"{re.escape(original)}"'
                updated_sql = re.sub(pattern, clean, updated_sql)
            
            # Create SQLite connection and execute
            conn = sqlite3.connect(':memory:')
            df.to_sql("a", conn, index=False, if_exists='replace')
            result_df = pd.read_sql_query(updated_sql, conn)
            conn.close()
            
            if len(result_df) == 0:
                return [["No results found"]]
            
            # Restore original column names
            result_columns = result_df.columns.tolist()
            restored_columns = []
            
            for col in result_columns:
                original_col = col
                for orig, clean in column_mapping.items():
                    if clean == col:
                        original_col = orig
                        break
                restored_columns.append(original_col)
            
            result_df.columns = restored_columns
            
            # Return as array
            output = [result_df.columns.tolist()]
            output.extend(result_df.values.tolist())
            
            return output
            
        except Exception as sql_error:
            return [["SQL Error:", str(sql_error)]]
            
    except Exception as e:
        return [["Table Error:", f"Exception: {str(e)}"]]

@xw.func
def sql_simple(data_range, sql_statement):
    """
    Simplified SQL function that works directly with data ranges (no table refs)
    Usage: =sql_simple(A1:C978721, "SELECT * FROM a WHERE [Firm / Headquarters] LIKE '%Texas%'")
    
    This is the most reliable version - pass the actual range, not table references
    """
    try:
        # Handle case where data_range might be a single cell with the SQL
        if isinstance(data_range, str):
            return [["Error:", "First parameter should be data range, second should be SQL"]]
            
        return sql_query_optimized_internal(data_range, sql_statement)
        
    except Exception as e:
        return [["Simple Error:", str(e)]]
    """
    Direct SQL processing without trying native first
    Usage: =sql_direct(A1:C978721, "SELECT * FROM a WHERE [Firm / Headquarters] LIKE '%Texas%'")
    
    This is equivalent to native =sql() but with better memory management
    """
    return sql_query_optimized_internal(data_range, sql_statement)

@xw.func
def sql_chunked(data_range, sql_statement, chunk_size=50000):
    """
    SQL processing with explicit chunking for very large datasets
    Usage: =sql_chunked(A1:C978721, "SELECT * FROM a WHERE [Firm / Headquarters] LIKE '%Texas%'", 50000)
    """
    return sql_query_chunked_internal(data_range, sql_statement, chunk_size)

def sql_query_chunked_internal(data_range, sql_statement, table_name="a", chunk_size=50000):
    """
    Internal chunked SQL processing
    """
    try:
        # Get headers
        headers = data_range[0]
        data_rows = data_range[1:]
        
        # Create in-memory SQLite database
        conn = sqlite3.connect(':memory:')
        
        # Process data in chunks
        total_rows = len(data_rows)
        
        for start_idx in range(0, total_rows, chunk_size):
            end_idx = min(start_idx + chunk_size, total_rows)
            chunk_data = data_rows[start_idx:end_idx]
            
            # Create DataFrame for this chunk
            chunk_df = pd.DataFrame(chunk_data, columns=headers)
            chunk_df = chunk_df.dropna(how='all')
            
            # Clean column names
            original_columns = chunk_df.columns.tolist()
            clean_columns = [re.sub(r'[^\w]', '_', str(col)) for col in original_columns]
            column_mapping = dict(zip(original_columns, clean_columns))
            chunk_df.columns = clean_columns
            
            # Append to SQLite table
            if_exists = 'replace' if start_idx == 0 else 'append'
            chunk_df.to_sql(table_name, conn, index=False, if_exists=if_exists)
        
        # Update SQL statement for clean column names
        updated_sql = sql_statement
        for original, clean in column_mapping.items():
            pattern = rf'\[{re.escape(original)}\]'
            updated_sql = re.sub(pattern, clean, updated_sql)
            pattern = rf'"{re.escape(original)}"'
            updated_sql = re.sub(pattern, clean, updated_sql)
        
        # Execute the SQL query
        result_df = pd.read_sql_query(updated_sql, conn)
        conn.close()
        
        if len(result_df) == 0:
            return [["No results found"]]
        
        # Restore original column names in results
        result_columns = result_df.columns.tolist()
        restored_columns = []
        
        for col in result_columns:
            original_col = col
            for orig, clean in column_mapping.items():
                if clean == col:
                    original_col = orig
                    break
            restored_columns.append(original_col)
        
        result_df.columns = restored_columns
        
        # Return as array
        output = [result_df.columns.tolist()]
        output.extend(result_df.values.tolist())
        
        return output
        
    except Exception as e:
        return [["Chunked Error:", str(e)]]
    """
    Internal function for SQL processing (same logic as before)
    """
    try:
        # Convert to DataFrame
        df = pd.DataFrame(data_range[1:], columns=data_range[0])
        df = df.dropna(how='all')
        
        # Clean column names for SQLite compatibility
        original_columns = df.columns.tolist()
        clean_columns = []
        column_mapping = {}
        
        for col in original_columns:
            clean_col = re.sub(r'[^\w]', '_', str(col))
            clean_columns.append(clean_col)
            column_mapping[col] = clean_col
        
        df.columns = clean_columns
        
        # Update SQL statement to use clean column names
        updated_sql = sql_statement
        for original, clean in column_mapping.items():
            pattern = rf'\[{re.escape(original)}\]'
            updated_sql = re.sub(pattern, clean, updated_sql)
            pattern = rf'"{re.escape(original)}"'
            updated_sql = re.sub(pattern, clean, updated_sql)
        
        # Create SQLite connection and execute
        conn = sqlite3.connect(':memory:')
        df.to_sql(table_name, conn, index=False, if_exists='replace')
        result_df = pd.read_sql_query(updated_sql, conn)
        conn.close()
        
        if len(result_df) == 0:
            return [["No results found"]]
        
        # Restore original column names
        result_columns = result_df.columns.tolist()
        restored_columns = []
        
        for col in result_columns:
            original_col = col
            for orig, clean in column_mapping.items():
                if clean == col:
                    original_col = orig
                    break
            restored_columns.append(original_col)
        
        result_df.columns = restored_columns
        
        # Return as array
        output = [result_df.columns.tolist()]
        output.extend(result_df.values.tolist())
        
        return output
        
    except Exception as e:
        return [["Error:", str(e)]]

@xw.func
def sql_query(data_range, sql_statement, table_name="a"):
    """
    Custom UDF to execute SQL on Excel data using SQLite
    Usage in Excel: =sql_query(A1:C978721, "SELECT * FROM a WHERE column LIKE '%Texas%'", "a")
    
    Args:
        data_range: Excel range with data (first row should be headers)
        sql_statement: SQL query string (use table_name as table reference)
        table_name: Name to use for the table in SQL (default: "a")
    
    Returns: Query results as array
    """
    try:
        # Convert xlwings range to pandas DataFrame
        df = pd.DataFrame(data_range[1:], columns=data_range[0])
        
        # Clean the data
        df = df.dropna(how='all')
        
        # Create in-memory SQLite database
        conn = sqlite3.connect(':memory:')
        
        # Write DataFrame to SQLite with specified table name
        df.to_sql(table_name, conn, index=False, if_exists='replace')
        
        # Execute the SQL query
        result_df = pd.read_sql_query(sql_statement, conn)
        
        # Close connection
        conn.close()
        
        if len(result_df) == 0:
            return [["No results found"]]
        
        # Convert result to list of lists with headers
        output = [result_df.columns.tolist()]
        output.extend(result_df.values.tolist())
        
        return output
        
    except Exception as e:
        return [["Error:", str(e)]]

@xw.func
def sql_query_chunked(data_range, sql_statement, table_name="a", chunk_size=50000):
    """
    Custom UDF to execute SQL on large Excel data using chunked processing
    Usage in Excel: =sql_query_chunked(A1:C978721, "SELECT * FROM a WHERE column LIKE '%Texas%'", "a", 50000)
    
    Args:
        data_range: Excel range with data
        sql_statement: SQL query string
        table_name: Name to use for the table in SQL
        chunk_size: Number of rows to process at once
    
    Returns: Query results as array
    """
    try:
        # Get headers
        headers = data_range[0]
        data_rows = data_range[1:]
        
        # Create in-memory SQLite database
        conn = sqlite3.connect(':memory:')
        
        # Process data in chunks to manage memory
        total_rows = len(data_rows)
        
        for start_idx in range(0, total_rows, chunk_size):
            end_idx = min(start_idx + chunk_size, total_rows)
            chunk_data = data_rows[start_idx:end_idx]
            
            # Create DataFrame for this chunk
            chunk_df = pd.DataFrame(chunk_data, columns=headers)
            chunk_df = chunk_df.dropna(how='all')
            
            # Append to SQLite table
            if_exists = 'replace' if start_idx == 0 else 'append'
            chunk_df.to_sql(table_name, conn, index=False, if_exists=if_exists)
        
        # Execute the SQL query
        result_df = pd.read_sql_query(sql_statement, conn)
        
        # Close connection
        conn.close()
        
        if len(result_df) == 0:
            return [["No results found"]]
        
        # Convert result to list of lists with headers
        output = [result_df.columns.tolist()]
        output.extend(result_df.values.tolist())
        
        return output
        
    except Exception as e:
        return [["Error:", str(e)]]

@xw.func
def sql_query_optimized(data_range, sql_statement, table_name="a"):
    """
    Optimized SQL UDF with better error handling and data type inference
    Usage in Excel: =sql_query_optimized(A1:C978721, "SELECT * FROM a WHERE [Firm / Headquarters] LIKE '%Texas%'")
    
    Note: Column names with spaces should be wrapped in square brackets in SQL
    """
    try:
        # Convert to DataFrame
        df = pd.DataFrame(data_range[1:], columns=data_range[0])
        df = df.dropna(how='all')
        
        # Clean column names for SQLite compatibility and create mapping
        original_columns = df.columns.tolist()
        clean_columns = []
        column_mapping = {}
        
        for col in original_columns:
            # Replace problematic characters in column names
            clean_col = re.sub(r'[^\w]', '_', str(col))
            clean_columns.append(clean_col)
            column_mapping[col] = clean_col
        
        df.columns = clean_columns
        
        # Update SQL statement to use clean column names if needed
        updated_sql = sql_statement
        for original, clean in column_mapping.items():
            # Replace bracketed column names: [Original Name] -> clean_name
            pattern = rf'\[{re.escape(original)}\]'
            updated_sql = re.sub(pattern, clean, updated_sql)
            
            # Replace quoted column names: "Original Name" -> clean_name
            pattern = rf'"{re.escape(original)}"'
            updated_sql = re.sub(pattern, clean, updated_sql)
        
        # Create SQLite connection
        conn = sqlite3.connect(':memory:')
        
        # Load data with appropriate data types
        df.to_sql(table_name, conn, index=False, if_exists='replace')
        
        # Execute query
        result_df = pd.read_sql_query(updated_sql, conn)
        conn.close()
        
        if len(result_df) == 0:
            return [["No results found"]]
        
        # Restore original column names in results if they match
        result_columns = result_df.columns.tolist()
        restored_columns = []
        
        for col in result_columns:
            # Find original column name
            original_col = col
            for orig, clean in column_mapping.items():
                if clean == col:
                    original_col = orig
                    break
            restored_columns.append(original_col)
        
        result_df.columns = restored_columns
        
        # Return as array
        output = [result_df.columns.tolist()]
        output.extend(result_df.values.tolist())
        
        return output
        
    except Exception as e:
        return [["Error:", str(e)]]

# Additional utility functions for common operations
@xw.func
def sql_count(data_range, where_clause="", table_name="a"):
    """
    Quick count with optional WHERE clause
    Usage: =sql_count(A1:C978721, "column LIKE '%Texas%'")
    """
    sql = f"SELECT COUNT(*) as count FROM {table_name}"
    if where_clause:
        sql += f" WHERE {where_clause}"
    
    result = sql_query_optimized(data_range, sql, table_name)
    
    if len(result) > 1:
        return result[1][0]  # Return just the count value
    return 0

@xw.func
def sql_distinct_count(data_range, column_name, where_clause="", table_name="a"):
    """
    Count distinct values in a column
    Usage: =sql_distinct_count(A1:C978721, "Firm / Headquarters", "column LIKE '%Texas%'")
    """
    clean_column = re.sub(r'[^\w]', '_', column_name)
    sql = f"SELECT COUNT(DISTINCT {clean_column}) as distinct_count FROM {table_name}"
    if where_clause:
        # Clean the where clause column names too
        clean_where = where_clause
        clean_where = re.sub(r'\b' + re.escape(column_name) + r'\b', clean_column, clean_where)
        sql += f" WHERE {clean_where}"
    
    result = sql_query_optimized(data_range, sql, table_name)
    
    if len(result) > 1:
        return result[1][0]
    return 0
